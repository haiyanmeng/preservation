\documentclass{acm_proc_article-sp}
\usepackage{url}
\begin{document}

\title{A Case Study in Preserving a High Energy Physics Application}
\author{
Haiyan Meng, Matthias Wolf, Peter Ivie, Anna Woodard, Michael Hildreth, and Douglas Thain\\
\affaddr{Department of Physics and Department of Computer Science and Engineering}\\
\affaddr{University of Notre Dame}\\
\affaddr{\{hmeng|mwolf3|awoodard|pivie|mhildreth|dthain\}@nd.edu}
}
\date{15 January 2014}
\maketitle

\begin{abstract}
...
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

\section{Introduction}

Reproducibility is a cornerstone of the scientific process.
In order to understand, verify, and build upon previous work,
one must be able to first recreate previous results by applying
the same methods. Historically, reproducibility this has been
accomplished through painstaking detailed documentation recorded
in lab notebooks, which are then summarized in peer-reviewed publications.
But as science increasingly depends on computation,
reproducibility must also encompass the environments, data, and software
involved in each result. It is widely recognized that informal
descriptions of software and systems -- although common -- are insufficient
for reproducing a computational result accurately.
A more automated and comprehensive approach is required.

The overall reproduction of a computation has three broad components,
each of which suggests somewhat different approaches:

\begin{itemize}
\item The {\bf computing environment}, consisting of the basic hardware and the operating system can be preserved as physical artifacts or as a combination of virtual machine monitor (hardware) and virtual machine image (operating system).
\item The {\bf scientific data} to be analyzed has historically received the most attention for curation.  In a large, well-organized project, it may be stored in a  data repository or database management system, with associated documentation and a curation strategy.  In a small effort, it could simply be a handful of files.
\item The {\bf software environment} includes the source code, binaries, scripts, configuration files, and everything else needed to execute the desired code.  As with data, the software could be drawn from a well-managed software repository, or it could be a handful custom scripts that exist in the user's home directory.
\end{itemize}

In a very abstract sense, reproducing a computation is trivial.
Assuming a computation is deterministic, one must simply
preserve all of the inputs to a computation, then re-run
the same code in an equivalent environment, and the same result
will be produced.  For a small custom application on a modest
amount of data, this could be accomplished by capturing the environment,
data, and software within a single virtual machine image,
and then depositing the virtual
it into a curated environment.  The publication could
then simply refer to the identifier of the image, which the
interested reader can obtain and re-use.  This approach has
been used to some success with systems X, Y, and Z.
\footnote{Of course, we are glossing over the problem that hardware
architectures and virtual machines also change, so one must also
preserve the VMM software necessary to run the image.  The VMM itself
depends on a software environment which must also be preserved.
A long-term preservation system might end up running a whole
stack of nested virtual machines in order to provide the desired
environment! }

However, this simple approach is not sufficient for large applications
that are run in complex social environments.

\begin{itemize}
\item There may be {\bf implicit dependencies} on items that are
not apparent to the end user.  For example, they may understand that
they rely on a particular data analysis package, but would have
no reason to know that the package has further dependencies on
other libraries and configuration files.  Or, they may know that
the computation only runs correctly on a particular machine, but
not know this is because it relies on data in a filesystem that
is mounted only on that machine.

\item The {\bf granularity} of the dependencies may not be well understood.
For example, the user may understand that a computation depends upon
a data collection that is 1TB in overall size, but not have detailed
knowledge that it only requires three files totalling 300MB out of that
whole collection

\item There may be dependencies upon {\bf networked resources} that
are inherently external to the system, such as a database, a code
repository, or a scalable filesystem.  For such resources, it
must be decided whether the dependency will simply be noted, or if it
must be incorporated whole or in part.

\item Where {\bf common dependencies} are widely used, it may be ineffecient or
impossible to store one copy of each dependency for each archived object.
Some form of sharing or de-duplication is necessary in order to keep
the archive to a reasonable size.
\end{itemize}

In this paper, we demonstrate many of these difficulties by presenting
a case study of a high energy physics application, considered from
the perspective of preservation.  The application is presented to us
first in the form of an email that describes in prose how to install
the software and run the analysis.  We perform several successive
refinements to convert it into an executable and preservable object.
Then, we develop three techniques for observing and capturing the
dependencies associated with the system, comparing the cost of capture,
the size of the preserved object, and the flexibility of the resulting
object.  We conclude with some reflections on the challenges of preservation
and advice for future efforts.

\section{Introduction of the exmaple}
\subsection{Background}
Matthias, one graduate student from Physics Department of University of Notre Dame, finished one interesting tau analysis. We want to understand how the tau analysis works and repeat it. 

Within the ongoing investigation of the Higgs boson at the CMS
detector, part of the LHC at CERN, the Higgs production in association
with two top quarks allows to measure the Higgs coupling strength to
top quarks.  As the Higgs boson is to short-lived to be detected
itself, it has to be reconstructed from its decay products.  The
analysis discussed in this section focuses on the Higgs decaying to
two tau leptons, which are themselves reconstructed from particle
showers.  Within the analysis, events are identified that show a
signature of decay products compatible with both hadronic tau and top
decays.  Properties of such events are used to distinguish signal-like
(matching simulated Higgs decays) and background-like events and are
also used in further statistical analysis.

The CMS collaboration provides analysis end-users with a pre-processed
and reduced data format, AOD, containing information for events, i.e.,
proton-proton collisions with a signature of interest, in the form of
reconstructed particles.  This format is based on the RAW output of
the CMS detector readout electronics and reconstructed world-wide.
Both real and simulated data are available for examination.

As AOD data are too large to be iteratively processed repetitively in
an physics analysis workflow, it is normally reduced further in
structural complexity and content.  For the analysis under
investigation here, this is a two-step process.  First, the AOD data
are processed at the Notre Dame working group cluster to BEAN events,
containing only trivial data containers packed in vectors.  This step
is time and CPU intensive and contains data of 11.6$\,$TB to be
analyzed by the tau analysis.  The BEAN format and data is shared
within the analysis group looking at Higgs production in association
with top quarks, which is formed by groups from a few American and
European universities.

In the second step, which is the beginning of the actual tau analysis,
the data are reduced to variables relevant to the tau analysis, while
only events matching basic quality criteria are kept.  This results in
a dataset of 43.3$\,$GB.  Again, the Notre Dame CMS groups cluster
resources are used to perform this reduction and selection, with
highly customized software.

The final data analysis, investigated below, can be run as a single
process, and contains a stringent event selection to keep only high
quality candidate events for the underlying physical process (using
122$\,$MiB of space).  Quantities from the selected events can be
both plotted and used in multivariate analysis to determine the level
of expected signal in real data.

% FWIW, units are TB, GB, not TiB, GiB!

\subsection{Workflow of Matthias's Example}
Declare environment variables

Obtain software from CMSSW

Obtain software from Git

Obtain software from some public HTTP web links

Obtain software from one private home page

Build software enviroment using SCRAM and Python

Install grid control, obtain CMS data from grid and store the data into HDFS mounted as one local file system.

Actual data analysis.

\subsection{Source and Size of Data and Software}
We notice that the software sources are rich, including CMSSW, Git, HTTP and one personal main page. The size of software from each source is shown in Table~\ref{table:software-source-size}.

As for the experiment data, Matthias copies the subset of T3 data from grid and stores the data into HDFS mounted as one local file system. The total size of data stored in HDFS is 33TB (*need to rethink).

\begin{table}
    \centering
    \begin{tabular}{|l|r|}
        \hline
        Source of Software & Size of Software \\ \hline
        cmssw\_5\_3\_11\_patch3 & 1.16TB \\ \hline
        Git & 51MB \\ \hline
        Public HTTP & 52MB \\ \hline
        Private home page & 41KB \\ \hline
    \end{tabular}
    \caption{Source and Size of Software}
    \label{table:software-source-size}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|l|}
        \hline
        Version 1\\ \hline
        \\
        1. Create CMS release,\\
            \hspace{9pt} e.g. cmsrel CMSSW\_5\_3... \\
        2. Install BEAN packages: \\
            \hspace{9pt} https://github.com/cms-ttH/...\\
        3. Install grid-control: \\ 
            \hspace{9pt} svn co https://dwekptrack... \\
        4. INstall the TauAnalysis package: \\
           \hspace{9pt} git clone https://github.com/matz-e/... \\
           \hspace{9pt} scram b -j32 \\
        5. fix grid\_control.cfg and run it. \\
        6. Perform actual analysis \\ 
        \\ \hline
        Version 2\\ \hline
        \\
        set CMSSW\_BASE = (CMSSW\_5\_3...) \\
        module load git \\        
        cmsenv \\ 
        cvs login \\
        cvs co UserCode/Bicocca/... \\
        git clone... \\
        scram b -j32 \\
        wget http://pyyaml.org/... \\
        python setup.py install... \\
        \#the experiment data is from HDFS \\
        roaster data/generic\_ttl.yaml \\ 
        \\ \hline
        Version 3\\ \hline
        \\
        \#the description of OS version and hardware architecture \\
        set CMSSW\_BASE = (CMSSW\_5\_3\_11\_patch3) \\ 
        cmsenv \\
        wget http://pyyaml.org/... \\
        python setup.py install... \\
        roaster data/generic\_ttl.yaml \\
        \\ \hline
        Version 4\\ \hline
        \\
        \#the description of OS version and hardware architecture \\
        \#declaration of environment variables\\
        \#clear data dependency\\
        \#clear software dependency\\
        \#software build and install command\\
        \#actual analysis program\\ 
        \\ \hline
    \end{tabular}
    \caption{Scripts of each Solution}
    \label{table:scripts}
\end{table}

\section{Solution 1 to Repeat the Program}
In order to repeat Matthais's example, the new user consulted the original author by email about the necessary work for the experiment. In response, Matthias introduced the general workflow of his tau analysis through one long email including notes, linux shell commands, web links. The First version of Table~\ref{table:scripts} illstrates his email.

However, this solution to repeat one experiment has three potential drawbacks. Firstly, the experience of repeating one tau analysis through emails is chaotic. You need constant jumps around multiple web links. There are overlap between the content of the email and the content of web links, which needs the new user to merge them. Multiple communication through emails is necessary to ensure the successful repeat of the whole analysis. For example, the original email refers to one environment variable called CMSSW\_base without clear declaration, the new user needs to send one email to the original writer to obtain its accurate value. Secondly, the necessary procedure to repeat the experiment, including software acquisition from different sources, is complex for the new user. In Matthias's example, the sources of software includes CMSSW, Git, HTTP. The access of CMSSW requires the new user be an authorized user of CMSSW. What's more, some parts of the workflow are unrepeatible. The third step of this experiment requires the new user owning the access authority to the Grid. 

Implication: Directly repeating the experiment using the workflow and results by the original author is complex and difficult. Some extra work must be done to make the reproduction process easier.

\section{Solution 2 to Repeat the Program}
One possible solution for the access authority problem of grid data is to grant the new user the authority to access gird data directly. Another possible solution is let the new user directly operate on the machine the original author used. As for the complexity of jumping between multiple web links, people may suggest that letting the original author generate one clear script including all the contents from different web links.

To test out these possible solutions, we integrate the content of all the notes, commands and web links involved in the emails into one neat, complete shell script, which begins with the definition of environment variables, software acquisition from CMSSW, Git and other web resources, and software installation, ends with the execution of the actual analysis program. Version 2 of Table~\ref{table:scripts} illstrates the merged script. As for the data acquisition from the Grid, we directly use the local copy in HDFS to avoidrequiring the new user to obtain a grid certification.

The breakdown of execution time of Solution 2 is shown in Table~\ref{table:time-2nd}. According to Table~\ref{table:time-2nd}, about half of the total execution time is consumed to prepare relevant software environment.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|}
    \hline
    Sub-Task & Time & Percentage \\ \hline
    Software preparation from CMSSW & 15min 30s & 25.27\% \\ \hline
    Software preparation from Git & 9s & 0.24\% \\ \hline
    Software preparation from Wget & 36s & 0.98\% \\ \hline
    Environment Build - SCRAM & 13min 20s & 21.74\% \\ \hline
    Environment Build - Python & 1s & 0.03\% \\ \hline
    Data analysis & 31min 44s & 51.74\% \\ \hline
    Total & 61min 20s & 100.00\% \\ \hline
    \end{tabular}
    \caption{Breakdown of Execution Time of Solution 2}
    \label{table:time-2nd}
\end{table}

The size of data and software from each category is shown in table~\ref{table:datasize-2nd}.

\begin{table}
    \centering
    \begin{tabular}{|l|r|}
    \hline
    Data Category & Data Size \\ \hline
    Software from CMSSW & 449MB \\ \hline
    Software from Git & 61MB \\ \hline
    Software from Wget & 52MB \\ \hline
    Hadoop & 33TB \\ \hline
    Total & 33792.562GB \\ \hline
    \end{tabular}
    \caption{Data Size of Solution 2}
    \label{table:datasize-2nd}
\end{table}

%*****hmeng-doult: the size of hadoop need to reconsidered. how to correctly illustrate the data size of this example?
Because the script includes all the necessary procedures for the reproduction of one analysis program, the readability and friendliness of this solution is higher than that of the email format. However, each execution of the script will involve the acquisition of software from different sources and the building of software environment. As for accessing grid data, using the data copy stored in the machine where Matthias executed the experiment requires the new user to have the authority to access the machine, which complicates the management of the original machine, even is impossible if the original machine executes rigid user access control. Granting everyone who wants to repeat the analysis the access authority for grid is unacceptable to system administrator. 

Implication: All the data and software involved in the experiment should be provided to the new user in the format of one self-contained package so that the multiple execution of the experiment from the same user only involve one time of data and software acquisition and environment building process. In addition, the requirement of the underlying OS and hardware should also be provided to the new user.

\section{Solution 3 to Repeat the Program}
\subsection{Working principle of Packaging Utility}
The difficulty of data access authority acquisition enforces us to find out one solution, in which the reproduction of the original analysis can be done without any external dependency. That is, one independent and self-contained package containing all the data and software dependencies is necessary. Someone may suggest that it should be the responsibility of the original author to generate the required package. However, letting the original author to provide the package which can be used by others to repeat the original experiment is unrealistic. One reason is that figuring out the underlying dependency of each software is complex and time-consumpting and even impossible for the original author. In this experiment, the machine used for the experiment is one public machine of physics department, and Matthias is one common user without root authority. The underlying OS and supporting softwares are installed and maintained by the IT department of the university. On the other hand, the architecture design of the required package including all the data and software dependencies is not under the research field of physists.

Parrot is a virtual filesystem access tool which attaching existing programs to a variety of remote I/O systems including http, ftp, gridftp, irods, HDFS, xrootd, grow and chirp. it traps all system calls of one program through ptrace debugging interface, and replaces them with remote I/O operations as desired. through executing one program under parrot, all the paths of files involved in this program can be recorded.  

With the help of parrot, one packaging utility which generates one independent package for one program to make the reproduction of the program convenient can be deployed. The basis of the packaging utility is one successful execution sandbox (the data from grid has been preserved in HDFS and the software from CMSSW, Git and HTTP has been on the local machine.). We re-execute the actual data analysis code under Parrot and get the name list of all the files actually accessed during the execution process of the actual data analysis. Then, according to the file name list, one package containing all the necessary data and software for one analysis program is generated. Next time, when another scholar wants to repeat the program, he only needs to obtain the package and directly execute the actual analysis program inside the package.

The shell script of the Solution 2 is simplified into one new version, which only contains the necessary environment variables, access authority and the actual analysis command. The Version 2 of Table~\ref{table:scripts} illustrates the simplified script.

\subsection{Workflow of Packaging Utility}
The workflow of generating one package for one analysis program is as follows:

(1) execute one analysis program under Parrot and obtain filename list (L1) of it

Parrot is one virtual file system that can support user access to multiple underlying file systems. Parrot traps each system call involved in the process of data access, figures out the type of file system and redirects it into corresponding operations to the accessed file system. During this process, each accessed file is recorded into one file with the access type of it, such as open, stat, read and write.

Command:

parrot\_run -l namelist /bin/tcsh parrot\_cms.sh

Output:
namelist size: 5.7MB     128711 lines

de-duplicated namelist size: 2.2MB   42385 lines

(2) generate one package containing the files of L1 

The packaging utility iterates each filename inside L1 and copies it into the target package. After completing the packaging process, the target package together with its description information, and one mountlist, which redirects the access of data from different file systems into the target package, will be provided to users. The packaging process also runs within Parrot environment to access data from different file systems.

Command:

parrot\_run /bin/bash package1-4.sh -l ~/namelist

the path of pacakge is: /tmp/package1-4

the total size of package is: 21GB

the total number of files is: 1204

the total number of directory is: 473

the total number of symbolic link files is: 195

the mountlist path is: /tmp/mountlist

%*****lack: file content + metadata; only file metadata; different process solutions
Each file inside L1 is copied into the package, the final path of the file becomes the path of the package, followed by the original file path. In this program, the path of the package is /tmp/package1-4, so the final path of one file with the original path path1 is /tmp/package1-4/path1.

To ensure the successful reproduction, the filesystem structure of the original execution environment should be preserved as completely as possible. However, attempting to copy the whole content of one directory or one file is space-consuming and time-consuming, because the original program may only access the metadata of one directory with the size of 200GB. our solution is to determine the copy degree of one directory or one common file according to the system call type for its path.

The map relationship between the file access path used in the actual analysis program and the actual file location used during the reproduction process is kept inside the mountlist file. the structure of the mountlist is as follows:

/ /tmp/package1-4 

/tmp/package1-4 /tmp/package1-4 

/dev /dev 

/misc /misc 

/net /net 

/proc /proc 

/sys /sys 

/var /var 

/selinux /selinux

%*****lack: re-execute process: re-execution process will redirect all the data access into the package except for data under proc sys dev selinux
(3) rerun the analysis program using the package

When another scholar wants to repeat one analysis program, the accessed data field is limited into the target package with the help of the mountlist generated in step 2. 

Command:

parrot\_run -m /tmp/mountlist /bin/tcsh parrot\_cms.sh

\subsection{Evaluation of Solution 3}
The breakdown of execution time of the Solution 3 is illustrated in Table~\ref{table:time-3rd}. The time used to obtain file namelist and generate P1 is greatly longer than the execution time within the new package. however, the time consumption of file namelist acquisition and package generation is one-time. That is, once the package is generated, many users can directly obtain the package and repeat the experiment separately. 

\begin{table}
    \centering
    \begin{tabular}{|l|r|}
    \hline
    Sub-Task & Time \\ \hline
    Obtain file namelist & 37min 51s \\ \hline
    Generate P1 & 13min 37s \\ \hline
    Re-run the program within P1 & 13min 5s \\ \hline
    \end{tabular}
    \caption{Time Breakdown of Solution 3}
    \label{table:time-3rd}
\end{table}

The data size of Solution 3 is shown in Table~\ref{table:datasize-3rd}. The data categories is more complex than our imagination. Except the data stored in Hadoop and the necessary set of software, files from /sbin, /bin and other paths are also necessary for the reproduction of the program.

\begin{table}
    \centering
    \begin{tabular}{|l|r|}
    \hline
    Data Category & Data Size \\ \hline
    Hadoop & 20GB \\ \hline
    CMSSW\_5\_3\_11\_patch3 & 5.2MB \\ \hline
    pscratch & 119MB \\ \hline
    usr & 60MB \\ \hline
    Others (/sbin + ... + /lib) & 28.4MB \\ \hline
    Total & 20.213GB \\ \hline
    \end{tabular}
    \caption{Data Size of Solution 3}
    \label{table:datasize-3rd}
\end{table}    

\subsection{Discussion of Solution 3}
Under Solution 3, the reproduction of one analysis program becomes easier.The namespace of repeating one experiment will be limited into one independent package containing all the data and software dependencies of one analysis program. 

Rethink Matthias's example, under Solution 3, the reproduction of it only needs the necessary environment variables, svn login,  the actual analysis command and the package. If different scholars want to repeat one analysis program, what they need to do is to obtain the package and rerun the actual analysis program. Under Solution 2, each scholar needs to get the necessary data and software, and then prepare software environment. 

\begin{table*}
    \centering
    \begin{tabular}{|l|r|r|r|}
        \hline
        Solution ID & Data Resources & Software Resources & Operation Manual \\ \hline
        Solution 3& grid & CMSSW Git HTTP & email \\ \hline
        Solution 3& local (HDFS) & CMSSW Git HTTP & shell script \\ \hline
        Solution 3& package & package & shell script + packaging utility \\ \hline
    \end{tabular}
    \caption{The relationship of different solutions}
    \label{table:relationship}
\end{table*}

%The fourth solution should be based on the third solution. during the packaging process, packaging utility first checks whether the file has been inside the target package, (if exists, whether it is the latest version). only if the file does not exist in the target package or the file in the target package is not the latest version, the packaging utility copies the file into the target package.

%*****hmeng-doubt: potential risk: different analysis programs share some common file names, but with different file contents. we should analyze the possibility of conflict: data.

%*****hmeng-doubt: in the fourth version, let user to mark whether they change the data and software. if changed, one new package is generated to avoid data overlap and pollution. otherwise, directly starts packaging process based on the current target package. 
\section{Comparison of different Solutions}
\subsection{Relationship of Different Solutions}
The relationship of these four solutions to repeat one program is shown in Table~\ref{table:relationship}.

\subsection{ Data size Comparison of Solution 2 and 3}
The packaging utility checks the system call of each file within the namelist, and maintains the minimum dataset, which makes the size of the final package is as small as possible. The total size of Hadoop under Solution 2 is astonishing, while the size of Hadoop under the package is decreased to 20GB. The same trend applies to the size of CMSSW\_5\_3\_11\_patch3. Because the packaging utility tries to construct one independent and self-contained package, necessary files  from /sbin, /lib and other directories are also copied into the package and denoted as "Other data for Package" in the Table~\ref{table:datasize-2nd3rd}.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|}
    \hline
     Data Category & Data Size & Data Size \\
    & Solution 2 & Solution 3\\ \hline
    Hadoop & 33TB = 33792GB & 20GB \\ \hline
     CMSSW\_5\_3\_11\_patch3 & 562MB & 5.2MB \\ \hline
     Other data for Package & 207.4MB & N/A \\ \hline
     Total & 33792.562GB & 20.213GB \\ \hline
    \end{tabular}
    \caption{Data Size Comparison beweeen Solution 2 and 3}
    \label{table:datasize-2nd3rd}
\end{table}

\subsection{Execution Time Comparison of Solution 2 and 3}
Table~\ref{table:time-2nd3rd} shows the execution time comparison between Solution 2 and 3. The time consumption of the reproduction of one program from different scholars kept the same under Solution 2, including software and data preparation. However, under Solution 3, the data and software preparation is one-time. The following reproduction of the same program only needs to obtain one copy of the package and execute the actual experimental analysis directly.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|}
    \hline
    Task Category & Execution Time & Execution Time \\
    & Solution 2 & Solution 3\\ \hline
    Software Acquisition & 16min 15s & N/A \\ \hline
    Environment Building & 13min 21s  & 4s \\ \hline
    Obtain file namelist & N/A & 37min 51s \\ \hline
    Generate the package & N/A & 13min 37s \\ \hline
    Actual Analysis & 31min 44s & 13min 1s \\ \hline
    Total & 61min 20s & N/A \\ \hline
    \end{tabular}
    \caption{Execution Time Comparison between Solution 2 and 3}
    \label{table:time-2nd3rd}
\end{table}    

%*****hmeng-doubt:Another reason for the packaging utility is that not all the data and software generated by the second version script is used during the the actual data analysis. The packaging utility can help us find out the optimal subset of data and software involved in one actual data analysis. 
%*****hmeng-doubt: this point is not the motivation. but one achievement comes together. out of imagination.
\section{Discussion of Data and Software Preservation}
\subsection{Preservation Integration of Multiple Programs}
The Solution 3 can work well for the preservation and reproduction of one single program. However, different experimenters may execute different analysis programs on the same dataset using the same or overlapping sets of software. Generating one new package for each analysis program from scratch is time-consuming and space-consuming, which makes one data and software preservation mechanism, that can integrate the preservation requirements of different analysis programs, become necessary. 


To integrate the data and software from multiple analysis programs into one package, the concrete information of data and software, such as size, version, source and the number of files, need to be recorded. we also need to design one more instructive shell script format, in which the data dependencies and software dependencies can be clearly expressed and recognized. The packaging process needs to be re-organized. packaging utility needs to maintain one list of current software and data subset already stored in the package. When one user wants to generate one package, it will fork the packaging utility, the packaging utility will scan the script, get the data and software dependency part, judge whether each dependency has been inside the package, and only add the new data and software into the package, and update the package information and relavant retrieval information.

\subsection{Preservation Granularity}
Another important factor of data and software preservation is the choice of preservation granularity. In Solution 3, the preservation granularity is one file, because Parrot virtual filesystem traps each system call and redirects the access path to each file. As a result, during the packaging process, each time only one file can be copied into the target package, which is low-efficient. However, there are other options of preservation granularity. The software is generally preserved in the unit of package inside the remote repository, the packaging process of one experiment can adopt one package as the unit. In addition, if the size of one whole software repository is small and the access frequency of each package inside it is high, the granularity can be set to the whole repository.

\subsection{User Access Model of Data and Software}
Through this case study, we notice that the user access module of remote data and software has a great impact of the preservation mechanism, especially when we try to integrate the preservation of multiple programs. If all the programs only refer the original data and software and refuse to modify them, the preservation is easy and can ignore the data version problem. However, if each program tries to modify the original data and software to accustom to its own requirement, the preservation mechanism must provide one way to differentiate the original data version and the special data version used in one certain program. To have a clear understanding of the data access model, more examples need to be investigated.

\section{Related Work }

\section{Conclusion}

this is an example of citing \cite{Laboratories79make}. 

\bibliographystyle{abbrv}
\bibliography{cclpapers,this}

\end{document}
