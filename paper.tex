\documentclass[procedia]{easychair}
\usepackage{url}
\usepackage[flushleft]{threeparttable}
\hyphenation{resour-ces}
\hyphenation{approac-hes}
\hyphenation{har-der}
\hyphenation{spe-cifically}
\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\title{An Invariant Framework for Conducting Reproducible Computational Science}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. When processed by
% EasyChair, this command is mandatory: a document without \titlerunning
% will be rejected by EasyChair

\titlerunning{An invariant framework for conducting reproducible computational science}

\author{
	Haiyan Meng\inst{1}
\and
	Matthias Wolf\inst{2}
\and
	Douglas Thain\inst{3}
}
\institute{
	Department of Computer Science and Engineering,
	University of Notre Dame,
	Notre Dame, Indiana, USA
	\email{hmeng@nd.edu}
\and
	Department of Physics,
	University of Notre Dame,
	Notre Dame, Indiana, USA
	\email{mwolf3@nd.edu}
\and
	Department of Computer Science and Engineering,
	University of Notre Dame,
	Notre Dame, Indiana, USA
	\email{dthain@nd.edu}
}

%  \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads. When processed by
% EasyChair, this command is mandatory: a document without \authorrunning
% will be rejected by EasyChair

\authorrunning{Meng, Wolf and Thain}

\begin{document}

\maketitle

\keywords{}

\begin{abstract}
\it Computational reproducibility depends on being able to isolate necessary and sufficient computational artifacts and preserve them for later re-execution.
Both isolation and preservation of artifacts can be challenging due to the complexity
of existing software and systems and the resulting implicit dependencies, resource distribution, and shifting compatibility of systems as time progresses---all conspiring
to break the reproducibility of an application. Sandboxing is a technique
that has been used extensively in OS environments for isolation of computational artifacts.
Several tools were proposed recently that employ sandboxing as a mechanism to ensure reproducibility.
However, none of these tools preserve the sandboxed application for re-distribution
to a larger scientific community---aspects that are equally crucial for ensuring reproducibility as sandboxing itself.
In this paper, we describe a combined sandboxing and preservation framework, which is efficient and practical and
useful for large-scale reproducibility. We present a case study of a complex high energy
physics application and show how the framework can be useful for sandboxing and preserving.
We report on the completeness, performance,
and efficiency of the framework, and suggest possible standardization approaches.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

\section{Introduction}

% Reproducibility
Reproducibility is a cornerstone of the scientific method~\cite{borgman2012data}. 
Its importance is underscored by its ability to advance science---reproducing by verifying and validating a scientific result leads to improved understanding, thus increasing possibilities of reusing or extending the result. 
Ensuring reproducibility of a scientific result, however, often entails detailed documentation and specification of the involved scientific method. Historically, this has been achieved through text and proofs in a publication. 
As computation pervades the sciences and transforms the scientific method, mere text is considered insufficient. 
In particular, apart from textual descriptions describing the result, a reproducible result must also include several computational artifacts, such as software, data,  environment variables, and state of computation that are involved in the adopted scientific method.  By including computational artifacts, the scientific result can be obtained again at a later time. 

Virtualization has emerged as a promising approach for reproducing computational scientific results. One approach is to conduct the entire computation relating to a scientific result within a virtual machine image, and then share the resulting image. This way VMIS become an authoritative, encapsulated and executable records of computations, especially computations whose results are destined for publication and/or re-use, and this can be shared easily. 
With this approach, oftentimes, the resulting image is too big in size for share-ability purposes. An alternative light-weight form of virtualization allows encapsulating just the application software, along with all its necessary dependencies into a self contained package, thus separating , and not including, the underlying operating system on which it is executed. This encapsulation is achieved by interposing application system calls, and copying necessary dependencies (data, libraries, code) into a layered filesystem. While both forms of virtualization provides an excellent mechanism for sandboxing the computations associated with a scientific result, neither form of virtualization provides any guarantee that the included pieces of software will indeed reproduce the associated scientific result. 

Since reproducibility is significantly about comprehensive documentation, virtualization approaches in its current form only make it easy to capture the computations, but not preserve the computation so that it is easy to understand, install, or alter the implicit dependencies that are part of computation, especially as dependencies and external software components evolve or become deprecated.
Software delivery mechanisms, such as Linux Containers and Docker have recently emerged that address several of the reproducible challenges that arise due to use of virtualization approaches. 

In this paper, we examine how software delivery mechanisms can be combined with traditional virtualization approaches for reproducibility, especially light-weight virtualization approaches. To conduct a thorough examination, we consider a real-world complex high energy physics application, independently developed by a group, but must be reproduced  so that the entire high energy physics community can benefit from the analysis obtained from the application. 
We describe challenges in reproducing this application, and consider the extent to which reproducibility can be satisfied with light-weight virtualization approaches and software delivery mechanisms. We then propose an invariant framework for computational reproducibility that combines light-weight virtualization with software delivery mechanisms. The framework uses light-weight virtualization for capturing a computational environment, but preserves the environment using a software delivery approach, so it can be re-executed later. By using standardized delivery mechanisms and expanding the application for future uses, the framework makes it invariant to temporal software changes that may occur in near future. We measure the performance overhead of combining light-weight virtualization with software delivery approaches. The combination imposes additional overhead due to double virtualization, but saves on context switches, since every system call is not interposed. 

The rest of the paper is organized as follows. Section \ref{} describes the high energy physics application and the challenges involved in reproducing it. 
Section \ref{} describes the light-weight virtualization and software delivery mechanisms, such as linux containers and dockers. 
In Section \ref{} we describe the combined invariant framework, consisting of capture and preservation phases. 
Section \ref{} describes how to employ capture, preserve, and distribute phases to reproduce the high-energy physics application.
Section \ref{} provides the experimental results. Finally, we conclude in Section \ref{}. 


-----
OLD TEXT

Reproducibility is a cornerstone of the scientific process~\cite{borgman2012data}.
In order to understand, verify, and build upon previous work,
one must be able to first recreate previous results by applying
the same methods. Historically, reproducibility has been
accomplished through painstaking detailed documentation recorded
in lab notebooks, which are then summarized in peer-reviewed publications.
But as science increasingly depends on computation,
reproducibility must also encompass the environments, data, and software
involved in each result~\cite{zabolitzky2002preserving}. It is widely recognized that informal
descriptions of software and systems -- although common -- are insufficient
for reproducing a computational result accurately.
A more automated and comprehensive approach is required.

The reproduction of a computation has three broad components,
each of which suggests somewhat different approaches:

\begin{itemize}
\item The {\bf computing environment}, consisting of the basic hardware and the operating system which can be preserved as physical artifacts or as a combination of virtual machine monitors (hardware) and virtual machine images (operating system)~\cite{matthews2009towards}.
\item The {\bf scientific data} to be analyzed has historically received the most attention for curation.  In a large, well-organized project, it may be stored in a  data repository or database management system, with associated documentation and a curation strategy.  In a small effort, it could simply be a handful of files.
\item The {\bf software environment} includes the source code, binaries, scripts, configuration files, and everything else needed to execute the desired code.  As with data, the software could be drawn from a well-managed software repository, or it could be a handful custom scripts that exist in the user's home directory.
\end{itemize}

In a very abstract sense, reproducing a computation is trivial.
Assuming a computation is deterministic, one must simply
preserve all of the inputs to a computation, then re-run
the same code in an equivalent environment, and the same result
will be produced.  For a small custom application on a modest
amount of data, this could be accomplished by capturing the
complete environment,
data, and software within a single virtual machine image,
and then depositing the virtual
image into a curated environment.  The publication could
then simply refer to the identifier of the image, which the
interested reader can obtain and re-use. This approach has
been used to some success with systems~\cite{castagne2013consider}.
\footnote{Of course, we are glossing over the problem that hardware
architectures and virtual machines also change, so one must also
preserve the VMM software necessary to run the image.  The VMM itself
depends on a software environment which must also be preserved.
A long-term preservation system might end up running a whole
stack of nested virtual machines in order to provide the desired
environment! }

However, this simple approach is not sufficient for large applications
that are run in complex environments.

\begin{itemize}
\item There may be {\bf implicit dependencies} on items that are
not apparent to the end user.  For example, they may understand that
they rely on a particular data analysis package, but would have
no reason to know that the package has further dependencies on
other libraries and configuration files.  Or, they may know that
the computation only runs correctly on a particular machine, but
not know this is because it relies on a filesystem that
is mounted only on that machine.

\item The {\bf granularity} of the dependencies may not be well understood.
For example, the user may understand that a computation depends upon
a data collection that is 1TB in overall size, but not have detailed
knowledge that it only requires three files totalling 300MB out of that
whole collection.

\item There may be dependencies upon {\bf networked resources} that
are inherently external to the system, such as a database, a code
repository~\cite{cms2006cmssw}, or a scalable filesystem~\cite{blomer2011cernvm}.  For such resources, it
must be decided whether the dependency will simply be noted, or if it
must be incorporated whole or in part.

\item Where {\bf common dependencies} are widely used, it may be inefficient or
impossible to store one copy of each dependency for each archived object.
Some form of sharing or de-duplication is necessary in order to keep
the archive to a reasonable size.
\end{itemize}

We do not claim to have solved these problems in any comprehensive
way.  Rather, our aim in this paper is to highlight the scope
of the problems by presenting a case study of one complex application.
The application is presented to us
first in the form of an email that describes in prose how to install
the software and run the analysis.  We perform several successive
refinements to convert it into an executable and preservable object.
We then develop techniques for reducing the size of the dependencies
that are necessary for the object to function, and we demonstrate
the preserved object functioning correctly in three different
physical and cloud environments.
We describe how each of these techniques may interact with
a future archive of preserved software artifacts, and conclude with
some reflections on the challenges of preservation and advice for future efforts.

\if 0
Section 2: Overview of Application
    CMS/LHC introduction.
    Data sources and reduction of size.
    Code sources and reduction of size.
    Prose observations about the script.
        Uses multiple repos that change over time, with varying level of stability.
        Low selectivity from the larger repos
        Significant initialization time to collect everything.
        Incidental infrastructure tools versus essential objects.
        Some dependencies were surprising.
    Figure: Diagram of app with both code and data sources.
    Table: Show all code and data sources and size within one table.

Section 3: Preservation Strategies
    Figure: Show app in four stages:
        Single email.
        Script with embedded references to dependencies.
        Script with map file that refers to external dependencies.
        Script with map file that refers to preserved dependencies.

    Transform to more suitable format that expresses dependencies.

    Incorporate into archive, saving deps and map file.

    But, how to get the dependencies?

    Three strategies:
        Original - Unmodified application run in original environment.
        Coarse-Grained - Capture deps at large granularity -- whole filesystems and repositories.
        Fine-Grained - Capture deps at a fine granularity -- individual files actually used.

    Coarse-Grained Method (Copy Repositories)
        First, determine dependencies
        Express app as script + map file
        Packaging tool downloads deps, rewrites map file.
        To run packaged application, obtain map, download

    Fine-Trained Toolkit (Parrot)
        Tool to detect dependencies (Parrot)
        Express app as script + map file
        Packaging tool downloads deps, rewrites map file.
        To run package application, run again with Parrot.

Section 4: Evaluation

    Table: Time and size of preserving using each of the two techniques.

    Explain why the techniques show different performance.

    Is one technique more effective than the other?  Why?
\fi

\begin{figure}[t]
\centering
\includegraphics[width=.5\textwidth]{data-code-size.eps}
\caption{Inputs to Tau Roast}
\label{fig:data-code-size}
\end{figure}


\section{Overview of Tau Roast}

Within the ongoing investigation of the Higgs boson at the CMS
detector, part of the LHC at CERN~\cite{collaboration2008cms}, the Higgs production in association
with two top quarks allows measuring the Higgs coupling strength to
top quarks~\cite{chatrchyan2013search}.  As the Higgs boson is too short-lived to be detected
itself, it has to be reconstructed from its decay products.

The application which is the study of this paper is called \emph{TauRoast}.
It searches for cases where the Higgs boson decays to two tau leptons.
Since the tau leptons are very short-lived, they are not observed directly, but by the particle decay products 
that they generate.  So, the analysis must search for detector
events that show a signature of decay products compatible with both hadronic tau and top decays.  Properties of such events are used to distinguish
the events of interest (Higgs decays) from all other events and
are also used in further statistical analysis.

Figure~\ref{fig:data-code-size} shows that both the code and data
that form \emph{TauRoast} are drawn from large repositories through
multiple steps of reduction.  A preservation strategy must weigh
whether to store the large repositories completely, the fragments
used by an artifact, or something in between.

{\bf Code Sources.} Like many scientific codes, the central algorithm
of \emph{TauRoast} is expressed in a relatively small amount of
custom code developed by the primary author.  But, the code cannot
run at all without making use of an enormous collection of software
dependencies.  Some of these dependencies are standard to operating
systems worldwide, some are standardized across the entire high-energy
physics field, some are particular to small collaborative groups,
and a few are very specific to a single researcher.

The largest of these repositories is the CMS Software Distribution (CMSSW)~\cite{cms2006cms},
a carefully-curated selection of software packages which is distributed
in several forms.  Historically, components of CMSSW were obtained by checking components
of the source out of CVS (CMSSW source codes were moved from CVS to git recently), or by installing a complete binary package on a shared
filesystem within an HPC center.  In recent years, distribution has moved to
an on-demand delivery system known as CVMFS~\cite{blomer2011cernvm}, which
provides a filesystem interface that transparently accesses a remote repository.
The content of CMSSW is managed very carefully by a centralized team whose main goal
is to ensure that the current version of the software operates correctly
on the operating systems and architectures currently in use.  However,
preservation is not a specific objective of the system, and so there is
no particular guarantee that old versions of CMSSW will continue to
operate indefinitely.

CMSSW contains many different tools, libraries, and utilities.  No single code uses anywhere close to all of these.  But, because it is widely used within the experimental researchers, it is common for users to simply expect that a particular version of the entire repository is available.
 
{\bf Data Sources.}
The CMS collaboration provides end-users with a pre-processed
and reduced data format, AOD~\cite{holtman2001cms}, containing information for events, i.e.,
proton-proton collisions with a signature of interest, in the form of
reconstructed particles.  This format is based on the RAW output of
the CMS detector readout electronics and reconstructed world-wide, which is then processed through various algorithms which derive signatures of individual particles.
Both real and simulated data are available for examination.

As AOD data are too large to be iteratively processed repetitively in
a physics analysis workflow, it is normally reduced further in
structural complexity and content.  For the analysis under
investigation here, this is a two-step process.  First, the AOD data
are processed at the Notre Dame working group cluster to BEAN (Boson Exploration Analysis Ntuple) events,
containing only trivial data containers packed in vectors.  This step
is time and CPU intensive and its output contains data of 11.6$\,$TB to be
analyzed by the tau analysis.
It is performed by a small custom code framework, which is built on top of CMSSW.
The BEAN format, production code, and
data are shared within the analysis group looking at Higgs production
in association with top quarks, which is formed by groups from a few
American and European universities,
consisting of up to a few dozen contributors.

In the second step, the data are reduced to the ``Ntuple'' format,
which contains only events matching basic quality criteria and
fields relevant to \emph{TauRoast}.
Again, the Notre Dame CMS groups cluster
resources are used to perform this reduction and selection,
running highly customized software,
built on CMSSW and the BEAN framework,
with code written and maintained by a small group.

Once the data has been reduced to Ntuples, TauRoast can be run as a single
process, and contains a stringent event selection to look only at high
quality candidate events for the underlying physical process.
Quantities from the relevant events can be
both plotted and used in multivariate analysis to determine the level
of expected signal in real data.
This package is written using the CMSSW build framework,
but only utilizes code from ROOT,
a particle physics toolkit underlying CMSSW,
and a few external python dependencies for convenience.

\section{Observations}

\emph{TauRoast} was provided to us in the form of an
email which described, in prose, how to obtain the source,
build the program, and run it correctly on one specific
machine at our home institution, with no particular guarantee that
it will run anywhere else in the world.
Although this starting point may seem extreme, it is
perfectly natural for collaborators to share configurations
with each other in this form, and to rely on the presence
of a working environment with appropriate dependencies already
installed.  From this starting point, the authors played the
role of curators, whose job it is to prepare the application
for permanent archival.

First, we elaborated the email instructions into an
executable script that obtains the dependencies and then
executes the analysis.  The script declares the necessary
environment variables, downloads and checks out the necessary source code,
builds it appropriately, calls initialization scripts in
the dependent software, and then runs the analysis.
A few rounds of correction with the original author were necessary
to obtain all the dependencies and run the artifact correctly.
(The original instructions introduced in the email also indicated how to run the application
within a production batch system.  For the purposes of preservation,
we consider the execution infrastructure to be distinct from the application,
and leave it out of consideration for now.)

The process of elaborating the program into a script revealed
several observations about this type of application:

\begin{itemize}

\item {\bf Many Explicit External Dependencies.}  TauRoast depends on a large number of
external dependencies, each with a different access method and data source.
While we knew in advance that it depended upon the large CMSSW distribution,
it was not apparent until elaborating the script that it depended upon
two different Github repositories for the Tau source,
a CVS server at CERN for some configuration information, a public web page
for the PyYAML library, and the public home page of a Notre Dame student
for one missing header file.  (The latter is particularly troubling!)
While, at some level, the authors and users of these software know of these dependencies, they are often missing in
informal communications or forgotten once the dependency is installed.
However, once known, they are at least expressed explicitly within the script.

\item {\bf Many Implicit Local Dependencies.} A much harder problem is that the
application assumed the presence of many different components in the local
filesystem view. It would be tempting to capture all of these by simply
storing a virtual machine image containing the local filesystem. However,
the application depended on no less than {\bf five} networked filesystems
available on a particular machine available to the author:
the data to be analyzed was stored on an HDFS~\cite{borthakur2008hdfs} cluster,
some configuration data was stored on a CVMFS~\cite{blomer2011cernvm} filesystem,
and a variety of software tools were on an NFS~\cite{howard1988scale},
PanFS~\cite{welch2008scalable} and AFS~\cite{sandberg1985design} systems.
The original authors were not aware of many of these dependencies,
because they simply relied on local administrators to configure the
software and make it available.

\item {\bf Configuration Complexity.}  As a means of controlling the complexity
of dependent software packages, the high energy physics community has developed
a number of tools that perform run-time configuration and consistency checks
of the available software.  {\tt scram} is the software management tool used
by the CMS experiment.  Before running any code, {\tt scram} is used to locate
the appropriate version of software,  set environment variables such as the PATH, run any
tool-specific configuration, and do the same for all software on which it depends.
If the correct versions are not available, {\tt scram} halts and emits an error.
While this procedure has great value for consistency, it also introduces a significant cost
because it involves a large number of nested scripts traversing a filesystem,
repeatedly looking up metadata.  In our example, the time to perform this configuration
with a cold cache is about 14 minutes, which is almost as large as the actual analysis
run, which takes 20 minutes.

\begin{figure*}[t]
\centering
\includegraphics[width=.8\textwidth]{version-evolution.eps}
\caption{Version Evolution}
\label{fig:version-evolution}
\end{figure*}

\item {\bf High Selectivity.}  Although the total size of the resources accessed by this
program is very large, the size of the data and software actually used are much smaller.
Often, an entire repository or data source is named within the script, but the program
only needs a handful of items from that source.  For example, the data is stored on an
HDFS filesystem with 11.6TB of data, but only 20GB are actually consumed by the program. 
The reason for this great reduction is at first each BEAN event contains a large amount of information, and the TauAnalysis throws away a lot of the irrelevant event information, keeping only the relevant bits.
The CMSSW repository is 88.1GB in total
but only 448.3MB of source is checked out, and the actually used software only
measured 6.3MB.  In a few cases, a source of software is named but never actually accessed.
(For example, our original script includes the Open Science Grid software stack in the PATH, but does not actually use it.)
We suspect that end users are accustomed to missing dependencies and thus get in the habit of adding commonly used software,
whether it is needed or not.

\item {\bf Rapid Changes in Dependencies.}  Over the course of three months
between collecting the initial email, analyzing the program, and writing this
paper, the computing environment continuously changed.  The CMSSW software
distribution released a new version, the target execution environment was upgraded
to a new operating system, and the application deprecated the use of CVS for obtaining
the software.  While the users of this software seem be accustomed to constant change,
any preservation technique will have to be very cautious about relying upon an
external service, even one that may appear to be highly stable.

\end{itemize}

\if 0

\begin{table}
    \centering
    \begin{tabular}{|l|}
        \hline
        {\bf Version 1: Email}\\ \hline
        1. Create a CMS release,\\
            \hspace{9pt} e.g. {\tt cmsrel CMSSW\_5\_3\_11\_patch3} \\
        2. Install the BEAN packages as the instructions: \\
            \hspace{9pt} {\tt \url{https://github.com/cms-ttH/BEAN/blob/...}}\\
        3. Install grid-control: \\ 
            \hspace{9pt} {\tt svn co \url{https://ekptrac.physik.uni-ka/...}} \\
        4. INstall the TauAnalysis package: \\
           \hspace{9pt} {\tt git clone \url{https://github.com/matz-e/...}} \\
           \hspace{9pt} {\tt scram b -j32} \\
        5. Fix grid\_control.cfg and run it. \\
        6. Perform the actual tau roast program. \\ 
        \hline
        {\bf Version 2: Script}\\ \hline
        {\tt setenv CMSSW\_BASE CMSSW\_5\_3\_11\_patch3} \\
        {\tt cmsrel \$HOME/\$CMSSW\_BASE} \\
        {\tt cvs co -r V03-09-23 PhysicsTools/PatUtils} \\
        {\tt git clone \url{https://github.com/cms-ttH/BEAN.git}} \\
        {\tt wget -r \url{http://nd.edu/~abrinke1/...}} \\
        {\tt scram b -j32} \\
        {\tt wget \url{http://pyyaml.org/download/pyyaml/PyYAML...}}\\
        \#the experiment data is from HDFS \\
        {\tt cd \$HOME/\$CMSSW\_BASE/src/PyYAML-3.10}\\
        {\tt cmsenv}\\
        {\tt python setup.py install --user} \\
        {\tt scripts/roaster data/generic\_ttl.yaml} \\ 
        \hline
        {\bf version 3: Formulated Script} \\ \hline
        {\tt setenv CMSSW\_BASE CMSSW\_5\_3\_11\_patch3} \\
        {\tt setenv {\bf GIT} \url{https://github.com}} \\
        {\tt setenv {\bf PYYAML} \url{http://pyyaml.org}} \\
        {\tt setenv {\bf ND} \url{http://nd.edu}} \\
        {\tt cmsrel \$HOME/\$CMSSW\_BASE} \\
        {\tt cvs co -r V03-09-23 PhysicsTools/PatUtils} \\
        {\tt git clone \${\bf GIT}/cms-ttH/BEAN.git} \\
        {\tt wget -r \${\bf ND}\url{/~abrinke1/ElectronEffectiveArea.h}} \\
        {\tt scram b -j32} \\
        {\tt wget \${\bf PYYAML}/download/pyyaml/PyYAML...}\\
        \#the experiment data is from HDFS \\
        {\tt cd \$HOME/\$CMSSW\_BASE/src/PyYAML-3.10}\\
        {\tt cmsenv}\\
        {\tt python setup.py install --user} \\
        {\tt scripts/roaster data/generic\_ttl.yaml} \\ 
        \hline
       {\bf Version 4: Fine-Grained Toolkit - Package}\\ \hline
        {\tt setenv CMSSW\_BASE CMSSW\_5\_3\_11\_patch3} \\
        {\tt setenv {\bf GIT} \url{https://github.com}} \\
        {\tt setenv {\bf PYYAML} \url{http://pyyaml.org}} \\
        {\tt setenv {\bf ND} \url{http://nd.edu}} \\
        {\tt cmsrel \$HOME/\$CMSSW\_BASE} \\
        {\tt cvs co -r V03-09-23 PhysicsTools/PatUtils} \\
        {\tt git clone \${\bf GIT}/cms-ttH/BEAN.git} \\
        {\tt wget -r \${\bf ND}\url{/~abrinke1/ElectronEffectiveArea.h}} \\
        {\tt scram b -j32} \\
        {\tt wget \${\bf PYYAML}/download/pyyaml/PyYAML...}\\
        \#the experiment data is from HDFS \\
        {\tt cd \$HOME/\$CMSSW\_BASE/src/PyYAML-3.10}\\
        {\tt cmsenv}\\
        {\tt python setup.py install --user} \\
        {\tt scripts/roaster data/generic\_ttl.yaml} \\ 
        \hline
    \end{tabular}
    \caption{Scripts of each Solution}
    \label{table:scripts}
\end{table}

\fi

\section{Evolving the Artifact}

It is clear that the artifact, as provided, is not in a suitable form
for preservation.  While it might be technically possible to automatically
capture the entire virtual machine and all of the connected filesystems,
it would require 166.8TB of storage, which would be prohibitively expensive
for capturing this one application alone.  Further, if multiple
similar applications are preserved, we would miss the opportunity to identify
common dependencies and store them once for multiple artifacts.
A more structured approach to dependency management is needed.

Figure~\ref{fig:version-evolution} shows how we have evolved this artifact
through several stages which make it more suitable for preservation.
In each step of evolution, we make the dependencies of the artifact
more explicit and available for analysis and automated processing.
As noted in the previous section, the original author provided us with
prose instructions by email which we translated into an
executable script.  The executable script has embedded in it
a number of external identifiers such as URLs pointing to repositories
and paths to networked filesystems.  As a general programming practice,
embedding such constants into the middle of a program is unwise, and so
we extract all of those identifiers and place them \emph{outside} the
script in a \emph{dependency map} or just \emph{map} for short.
The dependency map lists all of the external dependencies of the application, indicating
the type, how they are accessed, and where they are currently located.
The resulting script then simply refers to abstract file locations such
as \verb$GIT$ and \verb$CVMFS$, while the map file indicates where they
are currently located. If properly constructed, the script should not refer to any external
resource unless it is indicated in the dependency map.  We call this idealized
artifact an \emph{abstract script}.

By extracting the dependencies into the dependency map,
we introduce great freedom for the curator to move, transform, and otherwise
manipulate the dependencies of the artifact without damaging the artifact itself.
A Figure~\ref{fig:version-evolution} shows, it is straightforward for an
automated tool to examine
all of the dependencies in the map, download those that are missing,
and then modify the map to point to the local copies of the dependencies.
If we group the script, dependency map, and dependencies into a \emph{package},
we now have a self-contained artifact that can be moved from place to place.
In some cases, it may be safe to allow the dependency map to refer to 
trusted remote repositories.  Whether this is advisable is a judgment that
must be made by the user or the curator, taking into account the long-term
stability of said repositories.

However, only having the package and the dependency map is not enough. The successful execution of the application on the original machine also relies on the configuration of environment variables.
We collected the environment variables of the original machine and transformed it into one executable script. If another researcher wants to repeat the application, this executable script will be first executed. 
The environment variables refer to the dynamic named values defined in the range of one computer, like \verb!$PATH! and \verb!$PYTHONPATH!.
However, the dependency map refers to the actual storage location of path variables used in one abstract script. For example, the actual storage location of the path variable \verb!CVMFS! 
is {\tt /data/cvmfs/grid.cern.ch}.

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{solution3.eps}
\caption{Relationship of Roles}
\label{fig:solution3}
\end{figure}

The relationship of different roles involved in the application preservation and
reproduction is shown in Figure~\ref{fig:solution3}.  The original author uses the packaging utility to generate the package for one application.
Then the package, together with
its map file and description file will be published. When
another researcher wants to repeat the application, one copy of the package will be downloaded into the new machine and the application can be repeated.

When we try to repeat one application on one new machine, one map file is necessary for the relocation of the data access targets, as
show in Figure~\ref{fig:version-evolution}. 
The map file clearly defines the real location of each dependency in the format of dependency variables - the real location of dependency variable {\tt GIT} is \path{/data/git/cms-ttH} and the real location of dependency variable {\tt CVMFS}
is {\tt /data/cvmfs/grid.cern.ch}.
The script only refers to the dependency variables defined in the map file.
This design decouples the application script and the actual data access targets, which minimizes the impact of the evolution of different data dependencies
and ensures the transparent access.
The modification of the package only introduces the minimal changes of the map file on the client side.

This basic approach to dependency management is a step in the right direction
for dependencies that are \emph{explicit} and \emph{external} to the
user's native execution environment.  However, it leaves two other problems unsolved.

First, the basic approach requires that someone be \emph{aware} of the dependencies,
whether it be the end user, the system administrator, or the archive curator.
It seems reasonable to expect the user to be aware of a large dependency mentioned in a top-level script.
But, oftentimes the dependency is embedded invisibly deep within the software stack,
or is connected to the machine by the local system administrators.  No single party
is likely to have complete information about all of the dependencies.
Second, the basic approach assumes that the entire dependency is actually
consumed by the artifact.  As we have suggested above (and will show below),
this sort of program often only consumes a small fraction of what it does declare
as a dependency.

To address both of these problems, users and curators alike need tools that
will automatically observe the dependencies of complex applications, to
facilitate automatic and efficient preservation.

\section{Measuring Dependencies}

\begin{figure*}
\centering
\includegraphics[width=.8\textwidth]{workflow-parrot.eps}
\caption{Workflow of The Fine-Grained Dependency Management Toolkit Based on Parrot}
\label{fig:workflow-parrot}
\end{figure*}

We have developed a prototype tool to assist in the measurement and preservation
of implicit dependencies for complex applications.
We use Parrot~\cite{thain2005parrot} to explicitly record all
of the files accessed by our example application, allowing us to observe how
much of each external dependencies is used, and what local resources are implicitly used.
Using this information, we create a \emph{reduced package} which contains
only the files actually used by the application.

Parrot is a virtual filesystem access tool which has been used to attach
existing programs to a variety of remote I/O systems, such as HTTP, FTP, and CVMFS.
It works by trapping an application's system calls through the Linux {\tt ptrace} debugging
interface, and then replacing them with the desired I/O operations.  Parrot is already used
in the high energy physics community with applications like TauRoast specifically to 
provide access to the CMSSW software distribution via the CVMFS distributed file system.
We made small modifications to Parrot to record a \emph{namelist} which lists all 
of the files that an application actually accesses.

Figure~\ref{fig:workflow-parrot} illustrates the measurement process.
The starting point of this toolkit is one successful execution of the application on the native machine.
First, we execute the actual data analysis script under Parrot to generate the namelist.
Then, using the namelist, we generate a package containing all the necessary data
and software for one analysis program. When another
researcher wants to repeat the program, he only needs to obtain the package and
execute the actual analysis program inside the package. 

For one execution of \emph{TauRoast}, the generated namelist includes 132,047 accessed filenames,
along with the system calls used to access the file, such as {\tt open}, {\tt stat}, {\tt read}, etc.
With duplicate filenames removed, the list is reduced to 67,168 files.
Many of those entries do not exist, because they reflect attempts
by the application to search for programs and libraries in multiple places.
Only 22,068 entries reflect existing files or directories.

The packaging tool iterates over each item of the filename list, determines the process
mode and replication degree according to the file type (common files,
directories, symbolic links) and the system call type, generates one package
containing the dependencies, and summarizes the contents of the package
as shown in Table~\ref{table:package-info}.  To the extent possible,
the filesystem structure of the original environment is preserved.

We considered several approaches to constructing the package.
In a {\bf shallow copy}, we only copied the individual files in the namelist,
creating only parent directories for each.  Where a directory was listed,
we created the directory and populated it with empty files as placeholders
to facilitate a directory listing.  In a {\bf medium copy}, we copied the
individual files as before.  Where a directory was listed, we created
the directories and copied the contents of the files in that directory,
one level deep.  A {\bf deep copy} would duplicate all directories recursively,
but this would have resulted in TB-sized packages, so we did not consider
it further.

Parrot is required to re-run the packaged artifact, in order to force
the packaged files to appear to exist in their original locations.
To this end, the packaging tool creates a \emph{file map} which maps
the logical names of the files to their current physical locations, as shown in Table~\ref{table:map-file}.
Parrot reads the file map and redirects system calls at run-time to achieve the desired effect.
As the example suggests, special device files such as {\tt /proc} and {\tt /dev}
are not incorporated into the package but are instead accessed natively.

\begin{table}
    \centering
    \begin{tabular}{ll}
    \hline
    \bf Path used in Program & \bf Actual Location \\ \hline
    {\tt /} & {\tt /tmp/package-hep} \\ \hline
    {\tt /tmp/package-hep} & {\tt /tmp/package-hep} \\ \hline
    {\tt /dev} & {\tt /dev} \\ \hline
    {\tt ...} & {\tt ...}\\ \hline
    \end{tabular}
    \caption{Structure of Map File}
    \label{table:map-file}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{rrr}
\hline
                    & \bf Shallow Copy & \bf Medium Copy\\
\hline
    Whole Files    & 1632         & 15642\\ 
\hline
    Empty Files    & 14273        & 263\\
\hline
    Directories    & 1549         & 1549\\ 
\hline
    Symbolic Links & 4614         & 4614 \\
\hline
    \bf Total Size & \bf 21GB     & \bf 28GB \\ 
\hline
    \end{tabular}
    \caption{Package Information}
    \label{table:package-info}
\end{table}

\subsection{Tracking of Network Dependencies}

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{git-syscall.eps}
\caption{Exec Syscalls of a git command}
\label{fig:git-syscall}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{http_packet.eps}
\caption{HTTP Request and Response}
\label{fig:http_packet}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{url_redirection.eps}
\caption{An Example of URL Redirection}
\label{fig:url_redirection}
\end{figure}

All the dependencies of one program can be divided into three categories: local file system (e.g., /usr, /lib and /lib64), remote file systems which can be mounted as local directories (e.g., /cvmfs and /hdfs), and other remote network dependencies (e.g. http, https and ssh).
Through utilizing ptrace to trap each file-relevant system call used by a program, the first two categories of dependencies can be collected. 
Motivation of tracking network dependencies: Linkrot is a common and great threat to the preservation of scientific applications. A URL which works normally today may be unavailable permanently. A method which can help the scientists figure out all the network dependencies of their applications provides the chance to evaluate the stability of each network dependencies and preserve the unstable network resources before linkrot happens.
One direct solution to track the third category of dependencies is to utilize ptrace to check each exec system call used by a program and record each network-relevant executable and its parameters. However, an executable may create one or more child processes to finish some tasks, and a child process may create their own children processes. Figure~\ref{fig:git-syscall} shows all the processes created by a git command. A process with PID 998 is created to clone a remote git repository into local machine, which totally has 6 child processes. Moreover, a network-relevant executable name collection is necessary to identify network dependencies. The diversity of network-relevant executables and the large process tree generated for a network executable make this solution infeasible.

Another solution to track network dependencies is to track the network sockets. The user process communicates with the network protocol stacks in the kernel through the network socket layer user interface on Linux. From the process tree shown in the above figure, the process with PID 1000 responds to create a socket, connect to a DNS server, send a DNS request packet to the DNS server and receive a DNS response packet from the DNS server. The information gained from the network socket tracking can be divided into three categories.
\begin{itemize}
\item The information of network sockets used to communicate with remote network dependences (through socket and connect system calls) can be used to figure out the port number, service name (such as, http, https, and ssh), socket type (stream and datagram),  and the domain type (inet and inet6). 
\item The contents of DNS packets can be used to figure out the hostname and IP address of each remote network dependency. 
\item As for applications based on http protocol, all the http requests and responses can be seen, as shown in Figure~\ref{fig:http_packet}. However, as for applications based on https and ssh which encrypt network data using TLS/SSL, tracking network data on the socket level can only see the encrypted data. 
\end{itemize}

The results of tracking network sockets used by a program involves two categories of network resource redirections, dns-level hostname redirection and website-level url redirection. 
\begin{itemize}
\item dns-level hostname redirection: Each DNS packet includes five parts, header, question, answer, authority, and additional. A DNS response packet with the type CNAME has multiple answers. For example, the DNS response packet which tries to resolve {\tt www3.nd.edu} includes two answers. The first answer provides an alias of {\tt www3.nd.edu}, {\tt www-vip.cc.nd.edu}, and the second answer maps the alias to the IP address. 
{\tt www3.nd.edu} CNAME {\tt www-vip.cc.nd.edu}
{\tt www-vip.cc.nd.edu} A {\tt 129.74.12.151}
The CNAME-type DNS packets leave us a question, among the hostname used by the user, the alias(es), the IP address, what should we preserve?
\item website-level url redirection: Website-level url redirection happens when a website updates its location and still want their old users access their resources using the old url. Figure~\ref{fig:url_redirection} illustrates the url redirection between {\tt \url{www3.nd.edu/~ccl}} and {\tt ccl.cse.nd.edu}. Except for the difference of hostnames, the application layer protocols used by the old-version website and the new-version website is also different: the old website provides services through the https protocol, while the new website provides services through the http protocol. 
\end{itemize}

The usage of https protocol makes detecting of website-level url redirections through tracking system calls more difficulty and even impossible. All the application data will be encrypted at presentation layer through the TLS/SSL protocol, which is initialized at the session layer and works at the presentation layer.

\section{Evaluation}

We evaluated the correctness of the reduced package and the overhead of generating the package.
To do this, the application was repeated with the help of \emph{Original Script} (as shown in Figure~\ref{fig:version-evolution}) on the original machine. 
Then one \emph{reduced package} was generated and verified with the help of Parrot on the original machine.

Then, two different virtual machines (VM) - one VM from the Notre Dame Cloud Platform based on KVM and one VM from the Amazon EC2 Platform based on Xen, were employed to further verify the correctness of the reduced package.

We first repeated the example from scratch using the Script translated from the original email on the original machine and counted the time consumption and data size. 
Then based on the successful execution of Original Script, we evaluated Reduced Package - one package containing necessary dependencies is generated, and then the time consumption and data size is analyzed. 

To measure time consumption, we counted the time used to obtain remote software dependencies, build the environment, and analyze the dataset. We also counted the time used to obtain the namelist and to generate the package. To measure data size, we can easily figure out the size of each data and code source inside the package under Reduced Package.
Original Script does not support mining of implicit dependencies. As for remote sources, we can figure out the named size through the analysis script. However, it is hard to figure out the named size of each local source. 
Instead, we only knew that total size of each file system is very large.

Table~\ref{table:time-2nd3rd} shows the execution time comparison between
Original Script and Reduced Package.
Reduced Package is faster than Original Script, because all the software copied into the package has been compiled and the Software Acquisition stage is not necessary.
and all the environment building only takes 4 seconds,
We were surprised that Reduced Package even reduces the actual analysis time. 
The reason for this is that data is obtained through accessing HDFS in Original Script, but is copied into the package in Reduced Package. This localization of experimental data speeds up the data analysis process, resulting the actual analysis time reducing from 20 minutes to 13 minutes.

\begin{table}
    \centering
    \begin{tabular}{lrr}
    \hline
    \bf Task Category & \bf Original Script & \bf Reduced Package \\ \hline
    Obtain Namelist & N/A & 28min 28s \\ \hline
    Generate Package & N/A & 85min 51s \\ \hline
    Software Acquisition & 8min 11s & N/A \\ \hline
    Environment Build & 5min 49s  & 4s \\ \hline
    Analysis Code & 20min 31s & 13min 04s \\ \hline
    \end{tabular}
    \caption{Execution Time Comparison between Original Script and Reduced Package}
    \label{table:time-2nd3rd}
\end{table}    

Table~\ref{table:time-2nd3rd} also illustrates the time used to
obtain the namelist and to generate the package. 
The time used for these two steps is longer than the execution time,
because each filename of the list, together with its system call type, needs to be checked, and the structure of each directory item must be maintained.
However, 
this is only done once.
Once the package is
generated, many users can directly obtain the package and repeat the application 
separately. 

Table~\ref{table:size-original-real} on page 2 illustrates the total size, named size in the example and actually used size of each remote source (the first 4 items) and local source (the remaining 5 items).
The third column corresponds to the data size of Reduced Package and can be easily figured out, because all the necessary data has been copied into the package.
Original Script does not support measuring implicit dependencies. As for remote sources, we can figure out the named size through the analysis script. However, it is hard to figure out the named size of each local source. 
Instead, we only knew that total size of each file system is very large.

\begin{table}
	\centering
	    \begin{tabular}{llrrr}
	        \hline
	        \bf Name & \bf Location & \bf Total & \bf Named & \bf Used \\ 
	        \hline
	        CMSSW code     & CVS & 88.1GB & 448.3MB & 6.3MB\\ \hline
	        Tau source       & Git & 73.7MB & 73.7MB & 6.7MB \\ \hline
	        PyYAML binaries    & HTTP & 52MB & 52MB & 0KB \\ \hline
	        .h file       & HTTP& 41KB & 41KB & 0KB \\ \hline \hline
	        Ntuples data    & HDFS& 11.6TB & N/A& 20GB \\ \hline
	        Configuration & CVMFS & 7.4GB & N/A & 103MB \\ \hline
	        Linux commands & localFS & 110GB &  N/A & 68.4MB \\ \hline     
	        HOME dir& AFS &12GB & N/A & 32MB\\ \hline
	        Misc commands & PanFS & 155TB & N/A  & 1.6MB \\ \hline
	        Total      &    & 166.8TB            & N/A & 21GB \\ \hline
	    \end{tabular}
	    \caption{Data and Code Used by Tau Roast}
	    \label{table:size-original-real}
\end{table}
%	\begin{tablenotes}
%	      \small
%	      \item The first column illustrates the total size of each data and software source; 
%	            the second column illustrates the size of the named files from each source;
%	            the third column illustrates the size of actually used data from each source.
%	            N/A denotes it is hard to figure out the named size of implicit dependencies directly.        
%	    \end{tablenotes}
	    
To further verify the correctness of Reduced Package on other machines, two different machines are employed -
one virtual
machine~\cite{goldberg1974survey} from the Notre Dame Cloud Platform based on KVM sharing the same kernel version with the original machine,
and one virtual machine from  the Amazon EC2~\cite{amazon2010amazon} Platform based on Xen.

Table~\ref{table:config-vm} illustrates the configuration of 
each machine and the execution time of the application on each machine.
All the machines adopt the x86\_64 hardware platform and Linux kernel.
Both of the two VMs repeated the application with the help of the package generated on the original machine successfully.
The execution time on one machine greatly depends on its hardware configuration.

\begin{table}
    \centering
    \begin{tabular}{lrrrr}
    \hline
    \bf Machine Type & \bf Distro Version & \bf CPU Cores & \bf Mem (GB) & \bf Execution Time \\ \hline 
    Original Machine &  Red Hat 5.10 & 64 & 125 & 13min 04s\\  \hline
    KVM (Notre Dame) & CentOS 5.10 & 4 & 2 & 21min 38s\\ \hline
    Xen (EC2) & Red Hat 5.9 & 16 & 60.5 & 13min 30s\\ \hline
    \end{tabular}
    \caption{Evaluation of Different Machines}
    \label{table:config-vm}
\end{table}

In summary, we demonstrate that the application can be repeated with the help of the reduced package on one different machine with the required hardware architecture and OS kernel version.
%*****hmeng-doubt:Another reason for the packaging utility is that not all the data and software generated by the second version script is used during the the actual data analysis. The packaging utility can help us find out the optimal subset of data and software involved in one actual data analysis. 
%*****hmeng-doubt: this point is not the motivation. but one achievement comes together. out of imagination.

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{preservation-integration.eps}
\caption{Preserving Multiple Artifacts}
\label{fig:Preservation integration}
\end{figure}

\section{Open Problems}

Figure~\ref{fig:Preservation integration} shows the rough information
architecture of the archive that we imagine for complex
scientific software like TauRoast.
Each artifact to be preserved is a package that consists of a top-level
script to invoke the software, a dependency map, and the dependencies
themselves, which may be external to the original program.  The artifacts
are then ingested into the archive, where shared dependencies are stored
only once.  In cases where an artifact has a dependency on a trusted
(or very large) remote archive, the dependency may simply be tracked
. A researcher that wishes to reproduce a given
result need only refer to the unique identifier of the artifact, and will
be able to automatically extract all of the dependent components of
that artifact.

For example, in Figure~\ref{fig:Preservation integration} Script 1 depends on items A, B, and C.  Items A and B are ingested into the archive, where B is shared with Script 2.  Item C is stored in another trustworthy repository and is tracked rather than ingested.  When Script 1 is exported from the repository, items A and B are exported along with it, while C can be copied or remain remote, according to the end user's choice.

As simple as that picture appears, there are a number of problems that must be solved to get there:

{\bf Measure the Mess or Force Cleanliness?}  Two radically different approaches to dependency tracking are possible.  The first is to allow end users complete freedom to construct their environment as desired, then \emph{measure} what items were actually used.  As we have shown, this is possible, but has significant overheads and does not fully preserve the structure or intent of the end user.   The second approach is to \emph{force} users to work in a clean environment in which no resource can be used until a proper dependency has been declared.  This ensures that all dependencies are known in advance (and made explicit to the end user) but places a variety of restrictions on the user's daily work, and may prevent creative approaches that do not fit within the curator's view of how programs should be structured.  Whether end users will accept the inconvenience of forced cleanliness for the benefits of reproducibility can only be discovered through experiment.

{\bf Granularity of Dependencies.}  Dependencies could be handled
at many different levels of granularity.  In this work, we have shown
how they can be handled at the level of entire repositories or individual files.
Other possible choices might include intermediate-sized software packages
(like RPM) or in the case of experimental data, even portions of individual large files.  Clearly, a larger granularity will result in 
simpler dependency maps, and more wasted space; smaller granularity results
in complex dependency maps and less wasted space.  A hybrid solution may
be able to combine both by storing large granularity dependencies, but
selecting sub-items out of objects when efficiency demands it.

{\bf Scope of Reuse.}  We have presented the data preservation
problem as primarily one of accurate reproduction: if a result depends
on running program X, we must be able to run exactly X again.  However,
the goal of scientific reproducibility is rarely limited to running
\emph{precisely} what a predecessor did. Often, the objective is to
change a parameter or a data input in order to see how the result is affected.
To that end, the preservation system must capture enough of the surrounding
material to permit modifications to succeed.  From this perspective,
a larger granularity of preservation is desirable.  A better understanding of
how end users will consume preserved software will help to shape how
software is preserved in the first place.

{\bf Dependency Detection.}  If we allow users to work in uncontrolled
environments, then we must have better methods for understanding the dependencies  of existing programs.  At first, we relied on an expert reader to examine
the user's script and extract the dependencies.  This is clearly
not a scalable approach.  We then demonstrated an automated method of observing
what individual files are accessed by a program. However, this does
not cover all types of dependencies, particularly those that are networked.
More sophisticated observation techniques could infer higher-level information,
such as the RPM package names of the files accessed, the URLs of remote
repositories named throughout the program, or even the addresses and names of networked dependencies like databases and filesystems.

{\bf Source, Binary, or Both?}  A science archive might choose to retain
the source code of an artifact or the binary code that can actually run in a given environment.
While conventional wisdom suggests that access to source code is critical for the long term
survival and evolution of a piece of software, it also requires the maintenance of an enormous
amount of supporting software in the form of compilers, linkers, and supporting libraries for
the target platform.  Rebuilding all of these for every invocation of an artifact is
likely to have excessive cost.  We suggest that a realistic repository will have to
maintain \emph{both}: the source describes the ultimate meaning of the code, but
the binary is an important performance cache, a backup if the compiler toolchain should
fail to be preserved, and a checksum to ensure that a source artifact was rebuilt correctly.

\section{Related Work }

Generally, there are three approaches to preserve software environment:
hardware preservation, migration and emulation.  Hardware
preservation preserves the original software and its original operating
environment. 
Software migration technique~\cite{cifuentes1996binary,mancl2001refactoring} was used to facilitate running software on new machines.
However, migration often involves the re-compiling and re-configuring
the source code to accommodate a new hardware platform and software environment.
Emulation recreates the original software and hardware environment by
programming future platforms and OSs. One common solution to implement this is
virtual machine. According to the usage and emulation degree of the real
machine, virtual machine can be divided into system virtual machine and process
virtual machine. 
The working principle, design principle and
performance evaluation of system virtual machine were illustrated in~\cite{goldberg1974survey, smith2005architecture}. 
The
functionality of system VM to support different guest operating systems was illustrated in~\cite{barham2003xen,kivity2007kvm,rosenblum1999vmware}.
F. Esquembre~\cite{esquembre2004easy} illustrated how JVM, one process virtual machine, can expedite the creation of
scientific simulations in Java. 
The pros and cons of these three approaches were discussed in~\cite{matthews2009towards,phelps2005no,hong2010software}.

The preservation of computing environment and software environment was treated as one entirety in~\cite{matthews2009towards,phelps2005no,hong2010software}. However, frequently changing experiment software makes the maintenance of the preserved experimental environment very complex. 
CernVM~\cite{buncic2010cernvm} treated them as two different categories. The preservation of computing environment is implemented with CernVM, and the preservation of software environment is based on a CernVM filesystem(CVMFS) specifically designed for efficient software distribution.

The importance of preserving software in source code format was emphasized in~\cite{zabolitzky2002preserving,castagne2013consider}. 
However, CVMFS~\cite{buncic2010cernvm} published pre-built and configured experiment software releases to avoid the time-consuming software building procedure. 

Attempts from different perspectives to facilitate the reproduction of scientific experiments utilizing a preserved software library have been made. 
The software distribution mechanism over network was discussed in~\cite{compostella2010cdf, blomer2011cernvm}.
J. R. Rice et al.~\cite{rice1996scientific} made the reproduction process easier through the integration of user interface, scientific software libraries, knowledge base into problem-solving environment.
S. R. Kohn et al.~\cite{kohn2001divorcing} tried to enable the creation and distribution of language-independent software library by addressing language interoperability.
a scalable, distributed and dynamic workflow system for digitization processes was proposed in~\cite{schoneberg2013scalable}.
A distributed archival network was designed in~\cite{subotic2013distributed} to facilitate process-oriented automatic long-term digital preservation.
M. Agosti et al.~\cite{agosti2012envisage} aimed to help non-domain users to utilize the digital archive system developed for domain experts.

Current mechanisms of preserving scientific experiments assume that all the data and software mentioned in the experiments are necessary for the reproduction of the experiments. However, this is not always right. In some cases, the original author may leave additional code referring to irrelative data and software in the program. One mechanism, which can figure out the absolutely relevant data and software of one experiment, is important for both the preservation and reproduction of scientific experiments.

B. Matthews et al.~\cite{matthews2008significant} introduced one conceptual framework for software preservation from several case studies of software preservation.
One tool to capture software preservation properties within a software environment was designed in~\cite{matthews2010framework} through a series of case studies conducted to evaluate the software preservation framework.
L. R. Johnston et al.~\cite{johnston2014workflow} proposed one overall data curation workflow for 3-5 case studies of preserving research data.
Two case studies~\cite{borgman2012data} were conducted to figure out the properties of data to be reused in the future.

\section{Postscript}

We began this work in late 2013, generating a reduced package for \emph{TauRoast} based
on the configuration of a standard machine at Notre Dame at the time.  In the course
of writing this paper in spring 2014, almost everything about the computing environment
changed: the operating system was upgraded, a new version of CMSSW was released, and
our local HEP users switched from using CVS to CVMFS for accessing CMSSW.
The original script provided by the author failed to run on the same machine.
But, the reduced package we created three months earlier in the old environment
worked just fine on the new machine.

\section*{Acknowledgments}

This work was supported in part by National Science Foundation grants PHY-1247316 (DASPOS), 
OCI-1148330 (SI2) and PHY-1312842.
The University of Notre Dame Center for Research Computing scientists and engineers provided critical technical assistance throughout this research effort.

Copyright (C) 2014 The University of Notre Dame

This work is licensed under the Creative Commons Attribution 4.0 License.

\bibliographystyle{plain}
\bibliography{cclpapers,this}

\end{document}
