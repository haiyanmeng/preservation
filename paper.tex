\documentclass[procedia]{easychair}
\usepackage{url}
\usepackage[flushleft]{threeparttable}
\hyphenation{resour-ces}
\hyphenation{approac-hes}
\hyphenation{har-der}
\hyphenation{spe-cifically}
\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\usepackage{comment}
\excludecomment{TM}
%\setlength{\textwidth}{6.25in}

\title{An Invariant Framework for Conducting Reproducible Computational Science}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. When processed by
% EasyChair, this command is mandatory: a document without \titlerunning
% will be rejected by EasyChair

\titlerunning{An invariant framework for conducting reproducible computational science}

\author{
	Haiyan Meng\inst{2}
\and Rupa Kommineni\inst{1}
\and Quan Pham\inst{1} \\
\and Robert Gardner\inst{1}
\and Tanu Malik\inst{1}
\and
	Douglas Thain\inst{2}
}
\institute{
	Computation Institute,
	University of Chicago,
	Chicago, Illinois, USA \\
	\email{rupa, quanpt, rwg, tanum@uchicago.edu}
\and
	Department of Computer Science and Engineering,
	University of Notre Dame,
	Notre Dame, Indiana, USA \\
	\email{hmeng, dthain@nd.edu}
}

%  \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads. When processed by
% EasyChair, this command is mandatory: a document without \authorrunning
% will be rejected by EasyChair

\authorrunning{Meng et al.}

\begin{document}

\maketitle

\keywords{Preservation framework, reproducible research, virtualization, container}

\begin{abstract}
\it Computational reproducibility depends on being able to isolate necessary and sufficient computational artifacts and preserve them for later re-execution.
Both isolation and preservation of artifacts can be challenging due to the complexity
of existing software and systems and the resulting implicit dependencies, resource distribution, and shifting compatibility of systems as time progresses---all conspiring
to break the reproducibility of an application. Sandboxing is a technique
that has been used extensively in OS environments for isolation of computational artifacts.
Several tools were proposed recently that employ sandboxing as a mechanism to ensure reproducibility.
However, none of these tools preserve the sandboxed application for re-distribution
to a larger scientific community---aspects that are equally crucial for ensuring reproducibility as sandboxing itself.
In this paper, we describe a combined sandboxing and preservation framework, which is efficient, invariant and
practical for large-scale reproducibility. We present case studies of complex high energy
physics applications and show how the framework can be useful for sandboxing, preserving and distributing applications.
We report on the completeness, performance,
and efficiency of the framework, and suggest possible standardization approaches.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

\input {intro}
\input {tauroast}
\input {observation}
\input {evolution}
\input {measure}
\input {evaluation}
%\input {problems}
\input {related_work}
%\input {postscript}

\section*{Acknowledgments}

This work was supported in part by National Science Foundation grants PHY-1247316 (DASPOS), 
OCI-1148330 (SI2), PHY-1312842, ICER-1440327, SES-0951576 (RDCEP), and ICER-1343816 (UChicago subcontract).
The University of Notre Dame Center for Research Computing scientists and engineers provided critical technical assistance throughout this research effort.
The Open Science Grid at the University of Chicago provided critical technical assistance throughout this research effort.

\bibliographystyle{plain}
\bibliography{cclpapers,this}

\end{document}

\if 0
Section 2: Overview of Application
    CMS/LHC introduction.
    Data sources and reduction of size.
    Code sources and reduction of size.
    Prose observations about the script.
        Uses multiple repos that change over time, with varying level of stability.
        Low selectivity from the larger repos
        Significant initialization time to collect everything.
        Incidental infrastructure tools versus essential objects.
        Some dependencies were surprising.
    Figure: Diagram of app with both code and data sources.
    Table: Show all code and data sources and size within one table.

Section 3: Preservation Strategies
    Figure: Show app in four stages:
        Single email.
        Script with embedded references to dependencies.
        Script with map file that refers to external dependencies.
        Script with map file that refers to preserved dependencies.

    Transform to more suitable format that expresses dependencies.

    Incorporate into archive, saving deps and map file.

    But, how to get the dependencies?

    Three strategies:
        Original - Unmodified application run in original environment.
        Coarse-Grained - Capture deps at large granularity -- whole filesystems and repositories.
        Fine-Grained - Capture deps at a fine granularity -- individual files actually used.

    Coarse-Grained Method (Copy Repositories)
        First, determine dependencies
        Express app as script + map file
        Packaging tool downloads deps, rewrites map file.
        To run packaged application, obtain map, download

    Fine-Trained Toolkit (Parrot)
        Tool to detect dependencies (Parrot)
        Express app as script + map file
        Packaging tool downloads deps, rewrites map file.
        To run package application, run again with Parrot.

Section 4: Evaluation

    Table: Time and size of preserving using each of the two techniques.

    Explain why the techniques show different performance.

    Is one technique more effective than the other?  Why?
\fi

