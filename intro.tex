\section{Introduction}

% Reproducibility
Reproducibility is a cornerstone of the scientific method~\cite{borgman2012data}. 
Its importance is underscored by its ability to advance science---reproducing by verifying and validating a scientific result leads to improved understanding, thus increasing possibilities of reusing or extending the result. 
Ensuring reproducibility of a scientific result, however, often entails detailed documentation and specification of the involved scientific method. Historically, this has been achieved through text and proofs in a publication. 
As computation pervades the sciences and transforms the scientific method, mere text is considered insufficient. 
In particular, apart from textual descriptions describing the result, a reproducible result must also include several computational artifacts, such as software, data,  environment variables, and state of computation that are involved in the adopted scientific method.  By including computational artifacts, the scientific result can be obtained again at a later time. 

Virtualization has emerged as a promising approach for reproducing computational scientific results. One approach is to conduct the entire computation relating to a scientific result within a virtual machine image, and then share the resulting image. This way VMIs become an authoritative, encapsulated, and executable records of computations, especially computations whose results are destined for publication and/or re-use, and the VMIs can be shared easily \cite{}. 
With this approach, oftentimes, the resulting image is too big in size for share-ability purposes. An alternative light-weight form of virtualization allows encapsulation of the application software, along with all its necessary dependencies into a self-contained package, but does not make it executable.  It, thus, separates and does not include the underlying operating system on which the package is executed. The encapsulation of the self-contained package is achieved by interposing application system calls, and only copying the necessary dependencies (data, libraries, code, etc) into the package, which also makes it lighter weight than a VMI. While both forms of virtualization provide mechanisms for sandboxing the computations associated with a scientific result, neither form of virtualization provides any guarantee that the included pieces of software will indeed reproduce the associated scientific result. 

Since reproducibility is significantly about comprehensive documentation, virtualization approaches in their current form only make it easy to capture the computations. Preserving the computations, so that they are easy to understand, install, or alter implicit dependencies that are part of computation, especially as dependencies and software components evolve or become deprecated, is not effectively addressed. There are two approaches to address the preservation challenge. By either introducing tools that help document dependencies and provide software attribution within VMIs or packages, or alternatively by using software delivery mechanisms, such as, centralized package management, linux containers, and the, more recent, Docker framework. We examined the first approach previously in \cite{}. In this paper we examine the second approach, that is, examine how the documentation and preservation standards that are part of software delivery mechanisms can be combined with traditional virtualization approaches for reproducibility. In particular, we consider the light-weight virtualization approaches, because, we believe that the combination of light-weight approaches with more standardized software delivery mechanisms can lead to addressing the reproducibility challenge for a wide variety of scientific researchers. 
% they may help the This  We that must also include effective  to deliver the software to a wide audience. We examine the fire
%have recently emerged  several of the reproducible challenges that arise due to use of virtualization approaches. 

To conduct a thorough examination, we consider real-world complex high energy physics (HEP) applications, independently developed by two HEP groups, that must be reproduced so that the entire high energy physics community can benefit from the analysis obtained from the application. We describe challenges in reproducing the applications, and consider the extent to which reproducibility requirements can be satisfied with light-weight virtualization approaches and software delivery mechanisms. We propose an invariant framework for computational reproducibility that combines light-weight virtualization with software delivery mechanisms for efficiently capturing, invariantly preserving and practically deploying applications. In effect, the framework uses light-weight virtualization techniques for efficiently capturing a computational environment, but preserves the environment such that it is invariant to temporal software changes that may occur in near future and can be re-executed later. We measure the performance overhead of  light-weight virtualization and software delivery approaches. 
% For later. 
%The combination imposes additional overhead due to double virtualization, but saves on context switches, since every system call is not interposed. 

\if 0
The rest of the paper is organized as follows. Section \ref{} describes the high energy physics application and the challenges involved in reproducing it. 
Section \ref{} describes the light-weight virtualization and software delivery mechanisms, such as linux containers and dockers. 
In Section \ref{} we describe the combined invariant framework, consisting of capture and preservation phases. 
Section \ref{} describes how to employ capture, preserve, and distribute phases to reproduce the high-energy physics application.
Section \ref{} provides the experimental results. Finally, we conclude in Section \ref{}. 
\fi


%-----
%OLD TEXT
%
%Reproducibility is a cornerstone of the scientific process~\cite{borgman2012data}.
%In order to understand, verify, and build upon previous work,
%one must be able to first recreate previous results by applying
%the same methods. Historically, reproducibility has been
%accomplished through painstaking detailed documentation recorded
%in lab notebooks, which are then summarized in peer-reviewed publications.
%But as science increasingly depends on computation,
%reproducibility must also encompass the environments, data, and software
%involved in each result~\cite{zabolitzky2002preserving}. It is widely recognized that informal
%descriptions of software and systems -- although common -- are insufficient
%for reproducing a computational result accurately.
%A more automated and comprehensive approach is required.
%
%The reproduction of a computation has three broad components,
%each of which suggests somewhat different approaches:
%
%\begin{itemize}
%\item The {\bf computing environment}, consisting of the basic hardware and the operating system which can be preserved as physical artifacts or as a combination of virtual machine monitors (hardware) and virtual machine images (operating system)~\cite{matthews2009towards}.
%\item The {\bf scientific data} to be analyzed has historically received the most attention for curation.  In a large, well-organized project, it may be stored in a  data repository or database management system, with associated documentation and a curation strategy.  In a small effort, it could simply be a handful of files.
%\item The {\bf software environment} includes the source code, binaries, scripts, configuration files, and everything else needed to execute the desired code.  As with data, the software could be drawn from a well-managed software repository, or it could be a handful custom scripts that exist in the user's home directory.
%\end{itemize}
%
%In a very abstract sense, reproducing a computation is trivial.
%Assuming a computation is deterministic, one must simply
%preserve all of the inputs to a computation, then re-run
%the same code in an equivalent environment, and the same result
%will be produced.  For a small custom application on a modest
%amount of data, this could be accomplished by capturing the
%complete environment,
%data, and software within a single virtual machine image,
%and then depositing the virtual
%image into a curated environment.  The publication could
%then simply refer to the identifier of the image, which the
%interested reader can obtain and re-use. This approach has
%been used to some success with systems~\cite{castagne2013consider}.
%\footnote{Of course, we are glossing over the problem that hardware
%architectures and virtual machines also change, so one must also
%preserve the VMM software necessary to run the image.  The VMM itself
%depends on a software environment which must also be preserved.
%A long-term preservation system might end up running a whole
%stack of nested virtual machines in order to provide the desired
%environment! }
%
%However, this simple approach is not sufficient for large applications
%that are run in complex environments.
%
%\begin{itemize}
%\item There may be {\bf implicit dependencies} on items that are
%not apparent to the end user.  For example, they may understand that
%they rely on a particular data analysis package, but would have
%no reason to know that the package has further dependencies on
%other libraries and configuration files.  Or, they may know that
%the computation only runs correctly on a particular machine, but
%not know this is because it relies on a filesystem that
%is mounted only on that machine.
%
%\item The {\bf granularity} of the dependencies may not be well understood.
%For example, the user may understand that a computation depends upon
%a data collection that is 1TB in overall size, but not have detailed
%knowledge that it only requires three files totalling 300MB out of that
%whole collection.
%
%\item There may be dependencies upon {\bf networked resources} that
%are inherently external to the system, such as a database, a code
%repository~\cite{cms2006cmssw}, or a scalable filesystem~\cite{blomer2011cernvm}.  For such resources, it
%must be decided whether the dependency will simply be noted, or if it
%must be incorporated whole or in part.
%
%\item Where {\bf common dependencies} are widely used, it may be inefficient or
%impossible to store one copy of each dependency for each archived object.
%Some form of sharing or de-duplication is necessary in order to keep
%the archive to a reasonable size.
%\end{itemize}
%
%We do not claim to have solved these problems in any comprehensive
%way.  Rather, our aim in this paper is to highlight the scope
%of the problems by presenting a case study of one complex application.
%The application is presented to us
%first in the form of an email that describes in prose how to install
%the software and run the analysis.  We perform several successive
%refinements to convert it into an executable and preservable object.
%We then develop techniques for reducing the size of the dependencies
%that are necessary for the object to function, and we demonstrate
%the preserved object functioning correctly in three different
%physical and cloud environments.
%We describe how each of these techniques may interact with
%a future archive of preserved software artifacts, and conclude with
%some reflections on the challenges of preservation and advice for future efforts.

