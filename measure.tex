\section{Component Tools for Reproducible Research}

We describe some tools that can be used as part of the reproducible framework, and ensure invariance:

\subsection{Capture tools} For capture we focus on automated tools for capturing dependencies (both software and data) instead of manual tools such as makefiles and configuration scripts, since the latter cannot ensure invariance. 
%Parrot

\vspace{5pt}
\noindent {\bf Parrot} Parrot is a virtual filesystem access tool which has been used to attach
existing programs to a variety of remote I/O systems, such as HTTP, FTP, and CVMFS.
It works by trapping an application's system calls through the Linux {\tt ptrace} debugging
interface, and then replacing them with the desired I/O operations. 
To capture dependencies, Parrot records a \emph{namelist} which lists all 
of the files that an application actually accesses. Using the namelist, Parrot creates a \emph{reduced package} which contains
only the files actually used by the application.

Being tightly integrated with I/O systems, the Parrot virtual filesystem is required to be installed for both audit and execution to allow for reproducibility.
In the high energy physics community, Parrot is already used to 
provide access to the CMSSW software distribution via the CVMFS distributed file system.
and is thus invariant for applications like TauRoast. However, unless installed, Parrot cannot ensure invariance for general applications. 

% PTU
\vspace{5pt}
\noindent{\bf PTU} PTU creates a package of an application by recording program binaries, libraries, scripts, data files, and environment variables, etc. It also uses the UNIX \emph{ptrace} system call interposition to identify the code and data used by the running application. PTU was forked from CDE \cite{}, and was thus limited to major Linux kernel versions (e.g. 2.6.x); consequently we have removed that restriction by adapting it for the newly released Linux kernel 3.0 as well as for Mac OS X \cite{}. Thus PTU provides OS invariance and does not require installation and configuration. 

\subsection{Preservation tools}

% Parrot package
\vspace{5pt}
\noindent{\bf Parrot:} In Parrot, the package is preserved based on user option.  
In a {\bf shallow copy}, only individual files in the namelist are copied,
creating only parent directories for each.  Where a directory was listed,
a directory is created and populated with empty files as placeholders
to facilitate a directory listing.  In a {\bf medium copy}, 
individual files are copied as before and directory contents, one level deep are copied. A {\bf deep copy}, which can duplicate all directories recursively,
resulting in TB-sized packages, is not allowed. 

% PTU package
\vspace{5pt}
\noindent{\bf PTU:} Owing to CDE, in PTU, the complete package is preserved, consisting of all code and data files, however deep the data paths are. 
In CDE a binary only captures a single execution path, which is the execution path taken during run-time. If different execution paths need different types of dependencies, some dependencies may be left out. To preserve these dependencies, we have added addition functionality in PTU. In particular, information about other static binaries and required static shared libraries, such as version number, released version of shared libraries, and associated kernel distribution, is preserved using UNIX commands file, ldd, strings, and objdump \cite{}. We also have added a provenance wrapper over CDE \cite{}. This provenance wrapper documents all the dependencies, and provides information about process execution times and memory consumption. It creates a graph of the reference execution providing details about how data was created and transformed as part of the execution. The proevenance description is analgalous to the logically preserved unit. 

Neither the Parrot packages or extended CDE package provide a literate programming environment or a continuous integration environment for changes in software. Thus they are preserved but make break over time as technology changes.
 
 % Docker images
\vspace{5pt}
\noindent{\bf Linux Containers and Docker Images:} Linux Containers provide multiple isolated execution instances on top of the same kernel through OS-level virtualization. Thus they can be used to persist the captured packages into images. Using such containers, Docker now allows to preserve the image in a more user-friendly way. In particular, apart from performing Linux container (LXC) based operating system (OS) level virtualization, docker tools provides portable deployment of containers across platforms, documentation of packages in a scriptable format, and versioning of container images. The image can be preserved along with dockerfiles, which are plain text files with a literate programming environment: the computational part is similar to shell scripts and can help provision similar to other provisioning tools (e.g. Chef, Puppet) or Continuous Integration (CI) platforms (e.g. Travis CI), and the text part is for human consumption and more suited for use with a version management system such as subversion or git, which can track any changes made to the Dockerfile. Thus dockerfiles can be used to preserve the namelist of Parrot packages and provenance description in PTU. 
Docker is integrated with a continuous build environment which will check and validate the version of the software being used, and use a more recent version to build application software. 


\subsection{Deployment  Repositories}
Deployment repositories, such as Docker Hub, are distribution service for storing the pre-built images, along with their metadata, for download and reuse by others. While the architecture of such hubs is beyond the scope of the current paper, the existence of such a repository for reproducible research is vital to ensure security and longevity of preserved packages. 



\begin{figure*}
\centering
\includegraphics[width=.8\textwidth]{preservation_framework.eps}
\caption{Preservation Framework}
\label{fig: preservation_framework}
\end{figure*}

%\section{Measuring Dependencies}
%We have developed a prototype tool to assist in the measurement and preservation
%of implicit dependencies for complex applications.
%We use Parrot~\cite{thain2005parrot} to explicitly record all
%of the files accessed by our example application, allowing us to observe how
%much of each external dependencies is used, and what local resources are implicitly used.
%Using this information, we create a \emph{reduced package} which contains
%only the files actually used by the application.
%
%Parrot is a virtual filesystem access tool which has been used to attach
%existing programs to a variety of remote I/O systems, such as HTTP, FTP, and CVMFS.
%It works by trapping an application's system calls through the Linux {\tt ptrace} debugging
%interface, and then replacing them with the desired I/O operations.  Parrot is already used
%in the high energy physics community with applications like TauRoast specifically to 
%provide access to the CMSSW software distribution via the CVMFS distributed file system.
%We made small modifications to Parrot to record a \emph{namelist} which lists all 
%of the files that an application actually accesses.
%
%Figure~\ref{fig:workflow-parrot} illustrates the measurement process.
%The starting point of this toolkit is one successful execution of the application on the native machine.
%First, we execute the actual data analysis script under Parrot to generate the namelist.
%Then, using the namelist, we generate a package containing all the necessary data
%and software for one analysis program. When another
%researcher wants to repeat the program, he only needs to obtain the package and
%execute the actual analysis program inside the package. 
%
%For one execution of \emph{TauRoast}, the generated namelist includes 132,047 accessed filenames,
%along with the system calls used to access the file, such as {\tt open}, {\tt stat}, {\tt read}, etc.
%With duplicate filenames removed, the list is reduced to 67,168 files.
%Many of those entries do not exist, because they reflect attempts
%by the application to search for programs and libraries in multiple places.
%Only 22,068 entries reflect existing files or directories.
%
%The packaging tool iterates over each item of the filename list, determines the process
%mode and replication degree according to the file type (common files,
%directories, symbolic links) and the system call type, generates one package
%containing the dependencies, and summarizes the contents of the package
%as shown in Table~\ref{table:package-info}.  To the extent possible,
%the filesystem structure of the original environment is preserved.
%
%We considered several approaches to constructing the package.
%In a {\bf shallow copy}, we only copied the individual files in the namelist,
%creating only parent directories for each.  Where a directory was listed,
%we created the directory and populated it with empty files as placeholders
%to facilitate a directory listing.  In a {\bf medium copy}, we copied the
%individual files as before.  Where a directory was listed, we created
%the directories and copied the contents of the files in that directory,
%one level deep.  A {\bf deep copy} would duplicate all directories recursively,
%but this would have resulted in TB-sized packages, so we did not consider
%it further.
%
%Parrot is required to re-run the packaged artifact, in order to force
%the packaged files to appear to exist in their original locations.
%To this end, the packaging tool creates a \emph{file map} which maps
%the logical names of the files to their current physical locations, as shown in Table~\ref{table:map-file}.
%Parrot reads the file map and redirects system calls at run-time to achieve the desired effect.
%As the example suggests, special device files such as {\tt /proc} and {\tt /dev}
%are not incorporated into the package but are instead accessed natively.
%
%\begin{table}
%    \centering
%    \begin{tabular}{ll}
%    \hline
%    \bf Path used in Program & \bf Actual Location \\ \hline
%    {\tt /} & {\tt /tmp/package-hep} \\ \hline
%    {\tt /tmp/package-hep} & {\tt /tmp/package-hep} \\ \hline
%    {\tt /dev} & {\tt /dev} \\ \hline
%    {\tt ...} & {\tt ...}\\ \hline
%    \end{tabular}
%    \caption{Structure of Map File}
%    \label{table:map-file}
%\end{table}
%
%\begin{table}
%    \centering
%    \begin{tabular}{rrr}
%\hline
%                    & \bf Shallow Copy & \bf Medium Copy\\
%\hline
%    Whole Files    & 1632         & 15642\\ 
%\hline
%    Empty Files    & 14273        & 263\\
%\hline
%    Directories    & 1549         & 1549\\ 
%\hline
%    Symbolic Links & 4614         & 4614 \\
%\hline
%    \bf Total Size & \bf 21GB     & \bf 28GB \\ 
%\hline
%    \end{tabular}
%    \caption{Package Information}
%    \label{table:package-info}
%\end{table}

%\subsection{Tracking of Network Dependencies}
%
%\begin{figure}
%\centering
%\includegraphics[width=.6\textwidth]{git-syscall.eps}
%\caption{Exec Syscalls of a git command}
%\label{fig:git-syscall}
%\end{figure}
%
%\begin{figure}
%\centering
%\includegraphics[width=.6\textwidth]{http_packet.eps}
%\caption{HTTP Request and Response}
%\label{fig:http_packet}
%\end{figure}
%
%\begin{figure}
%\centering
%\includegraphics[width=.6\textwidth]{url_redirection.eps}
%\caption{An Example of URL Redirection}
%\label{fig:url_redirection}
%\end{figure}
%
%All the dependencies of one program can be divided into three categories: local file system (e.g., /usr, /lib and /lib64), remote file systems which can be mounted as local directories (e.g., /cvmfs and /hdfs), and other remote network dependencies (e.g. http, https and ssh).
%Through utilizing ptrace to trap each file-relevant system call used by a program, the first two categories of dependencies can be collected. 
%Motivation of tracking network dependencies: Linkrot is a common and great threat to the preservation of scientific applications. A URL which works normally today may be unavailable permanently. A method which can help the scientists figure out all the network dependencies of their applications provides the chance to evaluate the stability of each network dependencies and preserve the unstable network resources before linkrot happens.
%One direct solution to track the third category of dependencies is to utilize ptrace to check each exec system call used by a program and record each network-relevant executable and its parameters. However, an executable may create one or more child processes to finish some tasks, and a child process may create their own children processes. Figure~\ref{fig:git-syscall} shows all the processes created by a git command. A process with PID 998 is created to clone a remote git repository into local machine, which totally has 6 child processes. Moreover, a network-relevant executable name collection is necessary to identify network dependencies. The diversity of network-relevant executables and the large process tree generated for a network executable make this solution infeasible.
%
%Another solution to track network dependencies is to track the network sockets. The user process communicates with the network protocol stacks in the kernel through the network socket layer user interface on Linux. From the process tree shown in the above figure, the process with PID 1000 responds to create a socket, connect to a DNS server, send a DNS request packet to the DNS server and receive a DNS response packet from the DNS server. The information gained from the network socket tracking can be divided into three categories.
%\begin{itemize}
%\item The information of network sockets used to communicate with remote network dependences (through socket and connect system calls) can be used to figure out the port number, service name (such as, http, https, and ssh), socket type (stream and datagram),  and the domain type (inet and inet6). 
%\item The contents of DNS packets can be used to figure out the hostname and IP address of each remote network dependency. 
%\item As for applications based on http protocol, all the http requests and responses can be seen, as shown in Figure~\ref{fig:http_packet}. However, as for applications based on https and ssh which encrypt network data using TLS/SSL, tracking network data on the socket level can only see the encrypted data. 
%\end{itemize}
%
%The results of tracking network sockets used by a program involves two categories of network resource redirections, dns-level hostname redirection and website-level url redirection. 
%\begin{itemize}
%\item dns-level hostname redirection: Each DNS packet includes five parts, header, question, answer, authority, and additional. A DNS response packet with the type CNAME has multiple answers. For example, the DNS response packet which tries to resolve {\tt www3.nd.edu} includes two answers. The first answer provides an alias of {\tt www3.nd.edu}, {\tt www-vip.cc.nd.edu}, and the second answer maps the alias to the IP address. 
%{\tt www3.nd.edu} CNAME {\tt www-vip.cc.nd.edu}
%{\tt www-vip.cc.nd.edu} A {\tt 129.74.12.151}
%The CNAME-type DNS packets leave us a question, among the hostname used by the user, the alias(es), the IP address, what should we preserve?
%\item website-level url redirection: Website-level url redirection happens when a website updates its location and still want their old users access their resources using the old url. Figure~\ref{fig:url_redirection} illustrates the url redirection between {\tt \url{www3.nd.edu/~ccl}} and {\tt ccl.cse.nd.edu}. Except for the difference of hostnames, the application layer protocols used by the old-version website and the new-version website is also different: the old website provides services through the https protocol, while the new website provides services through the http protocol. 
%\end{itemize}
%
%The usage of https protocol makes detecting of website-level url redirections through tracking system calls more difficulty and even impossible. All the application data will be encrypted at presentation layer through the TLS/SSL protocol, which is initialized at the session layer and works at the presentation layer.

